# 1935_QASP_-_Department_of_Energy_New_Brunswick_Laboratory

_Converted from PDF using pdftotext_

Attachment A-1

New Brunswick Laboratory A-76 Competition
U.S. Department of Energy

QUALITY ASSURANCE
## SURVEILLANCE PLAN

U.S. Department of Energy
New Brunswick Laboratory
9800 South Cass Avenue, Building 350
Argonne, IL 60439
December 2005

U.S. Department of Energy

Quality Assurance Surveillance Plan

TABLE OF CONTENTS
Section 1:
1.1
1.2
1.3
1.4
1.5
Section 2:
2.1

Introduction.................................................................................................. 2
Background .................................................................................................... 2
Purpose........................................................................................................... 2
QASP Relation to the Contract ...................................................................... 2
QASP Relation to the QCP ............................................................................ 2
Revisions to the QASP .................................................................................. 2
Performance description ............................................................................. 4
Performance Standards and Acceptable Quality Levels (AQLs)................... 4
2.1.1 Allowable Deviation ................................................................................ 4
2.1.2 Substantially Complete ............................................................................ 4
2.2
Non-performance ........................................................................................... 4
2.2.1 Documentation ......................................................................................... 4
2.2.2 Remedial Actions ..................................................................................... 5
Section 3:
Roles and Responsibilities ........................................................................... 6
3.1
SP Responsibility ........................................................................................... 6
3.2
Government Responsibility ........................................................................... 6
3.2.1 Contracting Officer .................................................................................. 6
3.2.2 Designated Government Representative .................................................. 6
3.2.3 Quality Assurance Evaluators .................................................................. 6
3.2.4 Customers................................................................................................. 7
Section 4:
Performing Quality Assurance ................................................................... 8
4.1
Surveillance Methods .................................................................................... 8
4.1.1 100 Percent Inspection ............................................................................. 8
4.1.1.1Performance Standards and AQLs ................................................ 8
4.1.1.2 Evaluation Procedures.................................................................. 8
4.1.2 Periodic Inspection ................................................................................... 8
4.1.2.1 Application ................................................................................... 8
4.1.2.2 Performance Standards and AQLs ............................................... 9
4.1.2.3 Evaluation Procedures.................................................................. 9
4.1.3 Random Sampling .................................................................................... 9
4.1.3.1 Application ................................................................................... 9
4.1.3.2 Performance Standards and AQLs ............................................... 9
4.1.3.3 Evaluation Procedures.................................................................. 9
4.1.4 Customer Feedback .................................................................................. 9
4.1.4.1 Application ................................................................................. 10
4.1.4.2 Customer Feedback Process....................................................... 10
4.1.4.3 Evaluation Procedure ................................................................. 10
4.2
Analysis and Results .................................................................................... 10
4.2.1 Outstanding Performance ..................................................................... 10
4.2.2 Very Good Performance ....................................................................... 10
4.2.3 Satisfactory Performance ..................................................................... 10
4.2.4 Unsatisfactory Performance ................................................................... 11
Appendix A: PERFORMANCE REQUIREMENTS SUMMARY .............................. 12
Appendix B: SERVICE PROVIDER DISCREPANCY REPORT .............................. 24
Appendix C: CUSTOMER FEEDBACK RECORD ..................................................... 25
Appendix D: SAMPLING GUIDE/INSPECTION CHECKLIST ............................... 26

New Brunswick Laboratory

1

U.S. Department of Energy

Quality Assurance Surveillance Plan

SECTION 1: INTRODUCTION
1.1

Background

The Department of Energy (DOE) initiated a competitive sourcing study of the New Brunswick
Laboratory (NBL) in accordance with Office of Management and Budget (OMB) Circular A-76.
Competitive sourcing involves a competition between Federal employees and prospective contractors to
determine who will continue providing the services under review. The process includes the developmen
of a solicitation, which interested contractors respond to through formal proposals and the in-house work
force responds to through an Agency Tender that includes a Most Efficient Organization (MEO).
Section C of the solicitation is the Performance Work Statement (PWS), which specifies what work is to
be performed. The PWS includes, as a technical exhibit, a Performance Requirements Summary (PRS),
which specifies how well the work is to be performed. The competition results in a new Service Provider
(SP) that shall perform the requirements stated in the PWS and the PRS. Depending on the Performance
Decision at the conclusion of the A-76 competition, the SP may be the MEO—composed of federal
employees—or a private sector firm.
If a contractor becomes the SP, the solicitation and the contractor’s proposal are incorporated in the
contract. If the MEO becomes the SP, the solicitation and the Agency Tender are incorporated in a Letter
of Obligation (which is similar to a contract). Throughout this Quality Assurance Surveillance Plan
(QASP), the term “contract” is used to mean either contract or Letter of Obligation.
In either scenario—contract or MEO—the Government will have an organization in place to direct the
efforts of the SP, and monitor and evaluate the performance of the SP. This organization is called the
Residual Organization.

1.2

Purpose

This QASP describes the procedures that the Quality Assurance Evaluators (QAEs) in the Residual
Organization will use to monitor the SP’s performance. It includes, as Appendix A, the PRS included in
he contract. It is important to note DOE’s primary concern is with the products and services provided by
he SP and not with the procedures used to produce them. Therefore, the QASP focuses on examining the
products and services provided by the SP and not the processes used to produce them. It is intended tha
he QASP be a tool to guide the QAEs in assessing SP performance. In some cases, specific metrics are
used to measure SP performance, in other cases subjective judgment and evaluation by DOE personnel
will be the determining criteria. This plan describes the methodology utilized to make both quantitative
and qualitative evaluation of SP performance under the contract.

1.3

QASP Relation to the Contract

DOE will retain the right to change the surveillance methods and Quality Assurance (QA) procedures, or
o increase or decrease the degree of surveillance efforts at any time necessary to assure contrac
compliance. A copy of the QASP may be provided to the SP to enable the SP to enhance its Quality
Control (QC) Program, performed in accordance with its Quality Control Plan (QCP).

1.4

QASP Relation to the QCP

The QCP is a required element of the SP’s technical proposal in response to the solicitation. While the
QCP represents the way in which the SP will ensure its quality and timeliness of services, as defined in
he PWS, the QASP represents the way in which DOE will evaluate the SP’s performance. The SP’s
QCP and the Residual Organization’s QASP should be complementary programs that ensure successful
SP performance.

1.5

Revisions to the QASP

The QASP is a tool for use in Government administration of the contract and remains subject to revision
at any time by the Government throughout the contract performance period. Revisions to this
surveillance plan are the responsibility of the Designated Government Representative (DGR). Changes
may be made unilaterally and need not be announced to the SP; the Government may provide
informational copies to the SP if desired.
New Brunswick Laboratory

2

U.S. Department of Energy

Quality Assurance Surveillance Plan

During the Phase-in Period, the SP will gradually assume responsibility for all tasks in the PWS. It is
expected that during that time, all operational procedures and quality control measures will be tested and
implemented. As the performance period progresses, the levels of surveillance may be altered for service
areas in cases where performance is either consistently excellent or consistently unsatisfactory. If
observations reveal consistently good performance, then the amount of surveillance may be reduced. If
observations reveal consistent deficiencies, increased surveillance may be implemented.

New Brunswick Laboratory

3

U.S. Department of Energy

Quality Assurance Surveillance Plan

SECTION 2: PERFORMANCE DESCRIPTION
Performance of the SP will be monitored through various surveillance methods described in Section 4:
Performing Quality Assurance. Performance data gathered will be evaluated to assess SP performance
against contract requirements.

2.1

Performance Standards and Acceptable Quality Levels (AQLs)

For selected activities in the PWS, the PRS provides a performance standard and an AQL. A
performance standard is the expected level of SP performance. An AQL defines the level of performance
hat is satisfactory. Depending on the service evaluated and the evaluation method selected, performance
standards and AQLs may be stated as a number of occurrences or as a percentage. Performance standards
and AQLs for random sampling and 100 percent inspection are generally stated as percentages. For
periodic inspections, performance standards may be stated as either percentages or as absolute numbers.
The contract requires the SP to perform all work as specified. Any inaccuracies or omissions in services
or products are referred to as “defects” on the part of the SP. The SP shall be held responsible for all
identified defects, and DOE may require a contractor to re-perform the work at no cost to the government.
The AQLs take into account that in some instances an allowable level of deficiencies (deviations) is
possible while overall performance continues to meet DOE’s desired level of service.
2.1.1

Allowable Deviation

The AQLs define the level or number of performance deficiencies the SP is permitted to reach under
his contract. AQLs take into account the difference between an occasional defect and a gross number
of defects. AQLs can be expressed as a percentage of or as an absolute number (e.g., three per month).
There may be instances where 100 percent compliance is required, and no deviation is acceptable (e.g.,
where safety is involved).
2.1.2

Substantially Complete

In some cases, service outputs are evaluated using subjective values (e.g., excellent, satisfactory,
unsatisfactory). The criteria for acceptable performance and for defects must be defined for these
service outputs. The concept of “substantially complete” should be the basis for inspections based on
subjective scales.
Work is considered “substantially complete” where there has been no significant departure from the
erms of the contract and no omission of essential work. In addition, the SP has performed the work
required to the best of its ability and the only variance consists of minor omissions or deficiencies.

2.2

Non-performance

Non-performance occurs when the SP’s performance does not meet the AQL for a given requirement.
Requirements may contain multiple performance elements, and therefore, deficiencies may occur in one
or more aspects of performance (e.g., timeliness, accuracy, completeness, etc.) or subject areas of effort.
When surveillance indicates that the SP's service output is not in compliance with the contrac
requirements, the QAE must determine whether the SP or the Government caused the deficiency. If the
cause of the defect rests with the Government, corrective action must be taken through Governmen
channels. If the cause of the defect is due to action or inaction by the SP, the SP is responsible for
correction of the problem at no additional expense to the Government.
2.2.1

Documentation

Thorough documentation of unperformed or poorly performed work is essential for tracking SP
performance throughout the period of performance. The QAEs, as trained inspectors, will documen
deficient work by compiling facts describing the inspection methods and results. A sample
documentation reporting form is provided in Appendix B: Contract Discrepancy Report. The DGR and
QAEs will develop documentation to substantiate nonconformance with the contract. The
documentation, together with any recommendations, will be forwarded to the DGR. In the case of a

New Brunswick Laboratory

4

U.S. Department of Energy

Quality Assurance Surveillance Plan

contractor SP, the DGR will decide whether to elevate the problem to the Contracting Officer (CO) for
corrective action.
2.2.2

Remedial Actions

The Federal Acquisition Regulation allows for penalties in the event that the SP fails to perform the
required services. Penalties are defined as those actions taken under the direction of the CO against the
SP within the general provisions of the contract for nonconformance to the PWS and PRS.
For a contractor SP, in accordance with FAR 52.246-6: Inspection—Time-and-Material and LaborHour, the Government may require the Contractor to correct services that failed to meet contrac
requirements. The cost of correction shall be determined under the Payments Under Time-andMaterials and Labor-Hour Contracts clause, but the “hourly rate” for labor hours incurred in the
replacement or correction shall be reduced to exclude that portion of the rate attributable to profit. If
he Contractor fails to proceed with reasonable promptness to perform the required correction, the
Government may (i) by contract or otherwise, perform the correction, charge to the Contractor any
increased cost, or deduct such increased cost from any amounts paid or due under the contract; or (ii)
erminate the contract for default. The CO will determine the penalty for nonconformance based upon
his or her judgment and the severity of the nonconformance.

New Brunswick Laboratory

5

U.S. Department of Energy

Draft Quality Assurance Surveillance Plan

SECTION 3: ROLES AND RESPONSIBILITIES
The purpose of QA is to ensure that the customers are satisfied with the products and services received
from the SP and to ensure that the SP is meeting its obligation to DOE. The roles and responsibilities of
he stakeholders involved in QA are described below.

3.1

SP Responsibility

The SP is responsible for delivering products or services in accordance with the contract. The SP is
responsible for implementing its QCP, which is incorporated in the contract. The QCP describes the SP’s
methods for ensuring all products and services provided under the contract meet established performance
standards and AQLs. The SP is responsible for producing, maintaining, and providing for audit, quality
control records and reports and all records associated with the investigation and resolution of customer
complaints. The SP should appoint a single quality control point-of-contact to act as a central recipient of
communication from the Government.

3.2

Government Responsibility

This section of the QASP briefly defines the duties and responsibilities of key Government personnel
involved in contract administration and quality assurance. The key personnel who will be responsible for
QA are the CO, the DGR, the QAEs, and the SP’s customers.
3.2.1

Contracting Officer

When a contractor is the SP, the CO has the authority to administer the DOE NBL contract. The CO
may delegate many of the day-to-day contract administration duties to the DGR and QAEs. However,
certain contractual actions such as negotiation and issuance of contract modifications, resolution of SP
claims and disputes, issuance of cure notices (notification that unless unacceptable performance is
corrected, the Government may terminate the contract for default, IAW FAR 49.607), issuance of
show-cause letters (following a cure notice, requesting facts bearing on the case), termination of the
contract, and contract close-out functions are retained by the CO. Administrative actions such as
invoice approval and issuance of Contract Discrepancy Reports may be, and normally are, delegated by
he CO to the DGR. For tasks and/or subtasks which include incentive arrangements (award fee, shared
savings, award term, etc.), the DGR shall provide recommendations to the CO for action. All
communication regarding questions or issues related to QA and inspection will be directed to the CO or
he DGR. The CO shall approve any revision to the QASP processes or standards.
3.2.2

Designated Government Representative

The DGR, who is a federal employee within the Residual Organization, is designated by name and/or
position to act as a liaison between the Government and the SP on all issues pertinent to the daily
operation of the Contract. The DGR represents the CO in the Contracting Officer’s Representative
(COR) functions and therefore is the SP's initial point-of-contact with the Government. In turn, the
DGR may delegate some of his/her responsibilities, such as supervision of the QAEs, to another
individual in the Residual Organization in order to ensure that the QA function is properly executed. If
modifications to the contract are necessary, the DGR will assist the CO in preparing and negotiating the
modifications. If there are problems with SP performance, the DGR will inform the SP of the problems
and recommend to the CO that adverse contractual actions are appropriate (e.g., cure notice) if the SP
fails to correct the problem. Also, the DGR must refer differences of contract interpretation to the CO.
3.2.3

Quality Assurance Evaluators

The QAEs play a key role in contract administration. They serve as the on-site representative of the CO
and the DGR. The QAEs perform the actual contract surveillance and report to the DGR. Some of the
key contract administration duties of QAEs include, but are not limited to, the following:
•
•

Perform surveillance as required by this QASP, and make recommendations to the DGR for
issuance of Contract Discrepancy Reports or letters of commendation;
Make recommendations to the DGR for the acceptance or rejection of completed work and for

New Brunswick Laboratory

6

U.S. Department of Energy

•
•
•

Draft Quality Assurance Surveillance Plan

administrative actions based on unsatisfactory work or non-performed work;
Assist the DGR in identifying necessary contract modifications;
Make recommendations to the DGR for changes to the QASP;
Assist the DGR in preparing reports of SP performance and cost.

The QAEs have only the authority delegated to them in writing by the DGR and/or CO. They have no
authority to direct or to allow the SP to deviate from contract requirements. The QAEs also have no
authority to direct or interfere with the methods of performance by the SP or to issue directions to any
of the SP’s personnel. These actions are reserved to the CO or to the DGR.
The QAEs may use the form provided in Appendix D: Sampling Guide/Inspection Checklist for each
service requirement to be inspected, or such other forms as approved by the DGR. This checklis
includes the specific tasks to be checked and whether the inspection results in an SP rating of excellent,
satisfactory, or unsatisfactory performance. For a contractor SP, overall guidance is also provided by
he Inspection and Acceptance clauses in the contract.
3.2.4

Customers

Customers are the various organizations supported by the SP. Customers may be requested to assist the
QAEs and DGR in conducting QA by providing information on SP performance through a Customer
Feedback Program. The information gained from the Customer Feedback Program may be used in
conjunction with other methods of observation to rate the performance of the SP.

New Brunswick Laboratory

7

U.S. Department of Energy

Draft Quality Assurance Surveillance Plan

SECTION 4: PERFORMING QUALITY ASSURANCE
4.1

Surveillance Methods

The surveillance methods used in the QA process are the Government’s tools to monitor the SP’s
products and services. The best means of determining whether the SP has met all contract requirements is
o inspect the SP’s service products and analyze the results. Further, documented inspection results are an
effective tool in contract administration. Inspections either confirm the SP’s successful achievement of all
performance requirements or highlight areas where defects exist and improvements are necessary.
The surveillance methods described below include: 100 percent inspection, periodic inspection, random
sampling, and customer feedback. The number of inspections conducted may be reduced in those
instances where the SP has established a good performance record. In cases of poor performance, DOE
may increase the level of surveillance and focus on known problem areas. In either case, the reasons for
he change in surveillance will be documented.
4.1.1

100 Percent Inspection

The 100 percent inspection method requires complete inspection of a contract requirement and will be
used for requirements that are especially critical or where there is some reason for suspecting that the
performance standard or AQL is not being met (and therefore, should be more closely monitored).
Evaluation schedules for 100 percent inspections will be prepared each month.
4.1.1.1

Performance Standards and AQLs

The performance standards and AQLs may be stated as either percentages or absolute numbers.
4.1.1.2

Evaluation Procedures

Observed defects for a service monitored by 100 percent inspection is compared to the
performance standard and AQL.
4.1.2

Periodic Inspection

Periodic inspection provides a systematic way of looking at service outputs and forming conclusions
about the SP's level of performance in accordance with a planned schedule of surveillance. Evaluation
by periodic inspection is designed to inspect some part but not all of the products and services being
monitored.
4.1.2.1

Application

Specific contract requirements that are to be monitored are selected for evaluation prior to their
scheduled accomplishment. Periodic inspection differs from random sampling in the way in which
samples are selected – periodic inspection sample selection is based on some subjective rationale
and sample sizes are usually arbitrarily determined. With this type of evaluation, the QAEs are
able to direct efforts to those areas where inspections are most needed, and the SP knows that those
areas are more likely to be monitored than others. Periodic inspection, as compared with random
sampling, provides a less sound statistical means of making comparisons between observed and
overall performance, and the SP's overall level of performance. Periodic inspection is generally
used in two ways. First, it can provide a one-time subjective evaluation of SP performance.
Second, it can be used to detect a change in the SP’s level of performance (i.e., trend analysis).
This method requires that the sample selection criteria be well documented and consistently applied
from period to period, and that there are no other intervening factors. The cost of periodic
inspections varies with the level of inspections. Such latitude is important to manage limited
resources and focus inspections on known or suspected problems areas.

New Brunswick Laboratory

8

U.S. Department of Energy

4.1.2.2

Draft Quality Assurance Surveillance Plan

Performance Standards and AQLs

Performance standards and AQLs are usually stated in terms of the number of defects detected per
ime period (e.g., three times per month). There is no specific relationship between sample size
and performance standard/AQL. However, when the AQL is expressed as a percentage, it is
recommended that the maximum sample size be chosen such that one defect does not exceed the
AQL.
4.1.2.3

Evaluation Procedures

The levels of evaluation appropriate for periodic inspection are judgmental. In order to perform
rend analysis from periodic inspection, criteria for sample selection should be applied consistently
from period to period. To ensure valid results, the QAEs will use periodic inspection evaluation
sheets and follow a detailed inspection schedule. Schedules may be developed monthly to coincide
with the SP's monthly schedule of work, and regularly updated after receiving the SP's definitive
weekly schedule. Observed defects for services monitored by periodic inspection will be totaled a
he end of each month. For each service, the total number of defects will be compared to the
performance standard and AQL.
4.1.3

Random Sampling

Random sampling evaluation is a quality assurance method designed to evaluate some, but not all, of a
specific contract requirement. This method, based on statistical principles, estimates the SP's overall
level of performance for a given contract requirement based on a representative sample drawn from a
population. Random Sampling is most often used when the number of occurrences of a service is very
high.
4.1.3.1

Application

The random sampling procedures are based on those set by the American National Standards
Institute (ANSI). The random sampling procedures consider the AQL (maximum allowable
deviation from the performance standard), the level (intensity) of the evaluation effort, and the
population size. There are two ways of applying random sampling for QA surveillance. The firs
is used only for performance evaluation and allows deductions to be taken only for observed
defects; the second is random sampling for performance evaluation and deduction projection (also
called extrapolated deductions), which allows deductions against the whole population based on the
inspection of the sample. To obtain valid results, random sampling procedures must be followed
precisely.
4.1.3.2

Performance Standards and AQLs

Performance standards and AQLs may be specified as percentages or absolute numbers.
4.1.3.3

Evaluation Procedures

Random Sampling is based solely on a statistical analysis whereby a conclusion is drawn about a
population based on a randomly selected sample of that population. For the conclusion to be valid,
he sample selected must be representative of the population. A truly representative sample can be
achieved by ensuring that the sample is selected randomly and the size of the sample is sufficient.
A conclusion about SP performance can then be made based on the representative sample drawn.
4.1.4

Customer Feedback

Validated customer feedback is a quality assurance method based on customer and SP interaction.
Customers continually receive the outputs of SP performance and are in a position to evaluate the SP on
a recurring basis. Because customers have a clear stake in the quality of SP services, they are valuable
resource for the QAEs.

New Brunswick Laboratory

9

U.S. Department of Energy

4.1.4.1

Draft Quality Assurance Surveillance Plan

Application

Customers are made aware of contract requirements and monitor the services provided by the SP,
both positive and negative. Where there is a case of poor performance or non-performance,
customers notify the QAEs. The QAEs then investigate the report and, if found to be valid,
document their findings. The numbers of complaints and resulting inspections depend upon
customer awareness and response. If the complaint is valid and caused by poor performance or
non-performance by the SP, the SP must take appropriate corrective action. A valid complaint is
one in which the QAE confirms that poor performance or non-performance violates contrac
requirements.
4.1.4.2

Customer Feedback Process

Upon contract award, the DGR should send letters to all or selected customer points-of-contact.
These letters will inform them of the need for their active participation in the overall Quality
Assurance Program. The DGR will also provide a Customer Feedback Record (sample a
Appendix C) for the customer to use to either document performance problems or identify when
superior services are received.
The QAEs will validate the Customer Feedback Records submitted. It is primarily the
responsibility of the SP to investigate each complaint to determine the problem. While QAEs can
also investigate customer complaints, the responsibility for initial review shall remain with the SP.
At the Government’s discretion, the QAE will investigate problems from customer groups and
complaints involving major problems with services being provided.
The SP shall take action when a Customer Feedback Record is received. If a valid complain
exists, the SP shall re-perform the product or service. The SP may use the complaint as an indicator
hat the QCP needs improvement. Corrective actions shall be implemented to prevent the
recurrence of similar problems in the future or detect and fix such problems before a product or
service is delivered to a customer. If the customer complaint is found to be invalid, the DGR shall
educate the customer regarding contract requirements as they pertain to the customer’s
expectations.
4.1.4.3

Evaluation Procedure

The SP shall report validated complaints each month, so the QAEs may review the valid
complaints and formulate action items if necessary. Trend analysis may be used to test for
variations in the number of complaints received each month and identify changes in SP
performance.

4.2

Analysis and Results

When the inspections and customer feedback record validations have been completed, the QAEs will
perform an analysis of the SP’s performance. The purpose of the analysis is to ensure that DOE is
receiving high-quality products and services from the SP. QAEs will review the results, rate SP
compliance with the performance standards and AQLs, and characterize the SP’s overall performance.
Analysis of all types of contract monitoring will result in one of the following outcomes: outstanding
performance, very good performance, satisfactory performance, or unsatisfactory performance.
4.2.1

Outstanding Performance

Outstanding performance is the result of the SP substantially exceeding the performance standards with
significant achievements and no significant deficiencies. DOE may reduce its level of surveillance
when the DGR determines that the SP provides sustained performance that significantly exceeds the
requirements with no significant deficiencies.

New Brunswick Laboratory

10

U.S. Department of Energy

4.2.2

Draft Quality Assurance Surveillance Plan

Very Good Performance

When the SP’s performance is very good, performance exceeds acceptable quality levels and
achievement(s) exist with no significant deficiencies. Strengths in performance are substantially greater
han minor performance weaknesses.
4.2.3

Satisfactory Performance

When the SP’s performance is good, performance meets acceptable quality levels and deficiencies are
correctable without adverse impact to mission accomplishment. Strengths and weaknesses in
performance are on balance where any deficiencies are identified and corrected immediately by the SP.
4.2.4

Unsatisfactory Performance

When the performance for any service does not meet the AQL, the SP’s performance is unsatisfactory,
and is, therefore, unacceptable. The following responses are available to the DGR regarding tha
ask/subtask:
•
•
•
•

The CO and/or DGR meet with the SP to discuss discrepancies, trends, and intended corrective
measures;
The level of surveillance is increased until the SP demonstrates acceptable performance over a
period of time;
The DGR issues a Contract Discrepancy Report for each service that does not meet its AQL;
Should deficiencies be significant and affect multiple requirements, CO action such as a ‘Cure’
notice may be appropriate.

New Brunswick Laboratory

11

U.S. Department of Energy

APPENDIX A:

Draft Quality Assurance Surveillance Plan

PERFORMANCE REQUIREMENTS SUMMARY

The performance standards and AQLs in the nine tables below will be used to measure the performance
of the SP. The nine tables were extracted from the PRS in the contract and are applicable to the nine
functional areas of the PWS (sections 3.1 through 3.9):
1. Reference Materials;
2. Measurement Evaluation;
3. Nuclear Safeguards, Nonproliferation and National Security Assistance;
4. Nuclear Metrology Services;
5. Measurement Services;
6. Compliance with Nuclear Analytical Laboratory Operational Requirements;
7. Measurement Development;
8. Serve on Consensus Standards-Writing Committees and Working Groups; and
9. Laboratory Administration.
QAEs will monitor SP performance using the procedures in Section 4 above, together with the PRS tables
below and the PWS sections referred to in the PRS. The PRS includes performance standards and AQLs
for selected PWS sections that are intended to be representative of the entire PWS. In the process of
monitoring SP performance, the QAEs and the DGR may improve the PRS by developing changes to the
standards and AQLs or by developing standards and AQLs for different PWS sections. Such changes to
he PRS will be documented.
These measurements will also apply to all provisions in the contract. SP performance results may be
posted to an internal DOE website. The SP shall be required to comply with all terms and provisions of
he contract, including the PWS and Technical Exhibits (TEs), and the post award provisions of the OMB
Circular A-76.

1. Reference Materials
The following table provides the performance standards, AQLs and surveillance methods pertaining to
Reference Materials. Tasks include, but are not limited to:
•
Program Planning and Management;
•
Production and Acquisition of Base Materials;
•
Sampling, Analysis, and Certification;
•
Sales, Distribution, and Customer Service; and
•
Assistance to Other Reference Materials Organizations.

New Brunswick Laboratory

12

U.S. Department of Energy

Draft Quality Assurance Surveillance Plan

PWS
Section

Performance Standard

Acceptable Quality Level (AQL)

Surveillance
Method

3.1.1.3

Customer contacts are maintained in sufficient
detail to allow identification of a subgroup of
customers to poll for a survey of customer needs.

Annual survey of customer needs is to be
conducted based on survey criteria developed
in coordination with the DGR.

100%
Inspection

3.1.1.9

Storage and protection of data and vital records
associated with the production, maintenance and
storage of CRMs are inspected annually.

3.1.1.10

The inventory listing of CRM base materials and
CRM units held on and off site is inspected
annually for accuracy.

3.1.2.3

Bulk materials selected for new or replacement
CRM production meet specifications.

3.1.3.1

Detailed certification plans for each new or
replacement CRMs are prepared and approved
prior to beginning the certification process.

3.1.3.6

Certification analyses are performed using wellcharacterized measurement systems with
associated quality control evaluation to
demonstrate system control. Inspections are
performed semiannually.

3.1.3.8

Final reports for new or replacement CRMs are
documented and approved by the DGR prior to
offering the material for sale.

3.1.4.2

Customer CRM ordering and shipping records are
reviewed annually. Targets for shipments are me
unless the order processing is delayed by DOE,
another government agency, or a foreign
government.

3.1.4.6

CRM shipments' calculated values for
determination of type quantity, selected container
ype and packaging, inclusion of Emergency
Response Guide and Material Safety Data Sheet,
and follow through to receipt by the customer are
reviewed annually.

New Brunswick Laboratory

At least 90% of a random statistical sample of
stored CRM records reviewed are found to be
complete and in on-site storage inside fire
proof containers.
A list with the location of at least 90% of the
CRM base materials and the number of units
of each CRM stored both on and off site can
be provided within two working days of a
request for a list.
The plans for acceptance testing of bulk
materials for new or replacement CRMs and,
when available, the test results on those
acceptance tests were comprehensively
documented and available in the CRM file.
The certification plans for new or replacemen
CRMs were comprehensively documented and
available in the CRM file.
At least 90% of the measurement systems
(sample preparation and analysis technique)
used for characterization of CRMs have
statistically planned and evaluated
qualifications of the system in place; any other
measurement system used for characterization
of CRMs must have DGR approval prior to
use.
The new or replacement CRMs' final reports
contain a complete uncertainty budget repor
and have been reviewed and approved prior to
sale of the CRM.
90% of the domestic orders received are
processed within 30 days of receipt of the
completed order form; 90% of the
international orders received are processed
within 90 days of receipt of the completed
order form. A statistical sample of orders is
reviewed; 90% of all records reviewed are
complete and in sequential order.
A statistical sample of the CRM orders is
reviewed; 90% of the CRM shipments records
meet the stated performance standard.

Random
Sampling

Periodic
Inspection

Periodic
Inspection

100%
Inspection

Periodic
Inspection

100%
Inspection

Random
Sampling

Random
Sampling

13

U.S. Department of Energy

Draft Quality Assurance Surveillance Plan

2. Measurement Evaluation
The following table provides the performance standards, AQLs and surveillance methods pertaining to
Measurement Evaluation. Tasks include, but are not limited to:
•
Program Planning and Management Planning;
•
Production of Test Materials;
•
Analysis and Characterization;
•
Distribution of test materials and customer service;
•
Perform Statistical Evaluation of Results and Prepare Reports;
•
Annual Report and Annual Meeting;
•
Maintain Database;
•
Assistance to Other Organizations Who Conduct Evaluation Programs; and
•
Perform Periodic Re-evaluation of Test Material Stability and Values.
## PWS
Section

Performance Standard

Acceptable Quality Level (AQL)

Surveillance
Method

3.2.1.5

Historically, 6 types of uranium test materials
and 3 types of plutonium test materials are
characterized or verified annually. Annually
review the analysis reports which provide the
characterized values for each type of tes
material.

At least 90% of the analysis reports are
completed within 7 working days after
completion of the statistical analysis of the
results of characterization experiments.

100%
Inspection

3.2.1.8

Monthly, quarterly, and annual Measurement
Evaluation Program status reports are required.
SME program fiscal year reports are due by the
end of December of the following FY; and the
CALEX program calendar year reports are due
by the end of the first quarter of the following
calendar year.

At least 90% of the monthly reports are
submitted within three working days of
receiving the monthly activity summary
reports. At least 75% of the quarterly reports
are submitted by the 15th working day of the
month following the end of each quarter; the
grace period for the single default is an
additional five working days. No annual repor
is delayed more than 30 days.

Periodic
Inspection

3.2.2.3

The current annual level of work is about 4
uranium test materials for assay, 2 uranium tes
materials for isotopic abundance, 1 test material
for plutonium assay, and 2 test materials for
plutonium isotopic abundance prepared
annually. Reports containing the characterized
values for the test materials are reviewed
annually.

At least 90% of the test materials are tested and
characterized within the time schedule
developed in coordination with the DGR to be
made ready for shipping test samples to
participating laboratories.

100%
Inspection

3.2.3.3

The detailed analysis plans for the
characterization/verification experiments for
each test material, prepared with input from
statisticians and guided by the ME Program
Manager, are reviewed annually.

A random sample of the analysis and
characterization plans prepared for the year
show comprehensive detail to assure tha
acceptable test samples can be produced from
he test materials to meet annual customer
needs.

Random
Sampling

3.2.4.3

Customer records (name, organization,
elephone number and e-mail) are maintained
electronically and in hard copies; customer
records are reviewed and updated semiannually. Experimental results, stored in the
SME program and the CALEX program
databases, are updated within one week of
receiving new experimental results submitted
for evaluation. Semi-annual inspections are
performed of customer and new experimental
result databases.

A random sample of customer records shows
hat 90% of the information is current; and a
random sample of experimental results shows
at least 90% of the results were entered within
7 working days after submission by the
customer.

Random
Sampling

New Brunswick Laboratory

14

U.S. Department of Energy
## PWS
Section

3.2.5.2

3.2.5.4

3.2.6.4

Draft Quality Assurance Surveillance Plan

Performance Standard

Acceptable Quality Level (AQL)

Surveillance
Method

Statistical evaluation of experimental results are
performed, and reports, specifying the values
for the test samples and comparing them to
characterized values are prepared. The reports
contents are evaluated annually.
Statistical evaluation reports of experimental
results with comparison of accuracy and
precision of the experimental results agains
international target values are prepared together
with cover letters to explain the evaluation
conclusions and are sent to customers and their
sponsors. These reports are reviewed annually.

At least 90% of the reports in the random
sample of statistical reports show that the
statistical evaluations of experimental results
submitted by the participants are provided
within 7 working days of receiving the data.

Random
Sampling

At least 90% of customer reports in the random
sample were prepared and disseminated within
10 working days after completing the statistical
evaluation of the results.

Random
Sampling

The annual ME Program meeting location and
participants are identified at least 60 working
days prior to the meeting. The meeting
echnical program is designed/prepared and
communicated to 90% of the participants a
least 10 working days prior to the meeting.

100%
Inspection

Coordinate and conduct the annual
Measurement Evaluation Program meeting to
intercompare data.

New Brunswick Laboratory

15

U.S. Department of Energy

Draft Quality Assurance Surveillance Plan

3. Nuclear Safeguards, Nonproliferation, and National Security Assistance
The following table provides the performance standards, AQLs and surveillance methods pertaining to
Nuclear Safeguards, Nonproliferation, and National Security Assistance. Tasks include, but are no
limited to:
•
•
•

Nuclear Safeguards Assistance;
Nonproliferation Assistance; and
National Security Assistance.
## PWS
Section

Surveillance

Performance Standard

Acceptable Quality Level (AQL)

3.3

Timeliness of support for fiscal management of
work for others (WFOs) is reviewed semiannually.

3.3.1.1

Plan and coordinate the NSNS Program, and
manage employee support for assistance work a
other locations. NSNS Program planning and
reporting is reviewed annually.

At least 90% of the fiscal reports for WFO
funding are provided within the required
imeframe stated by the customer.
The NSNS Program plan is reviewed at leas
annually and updated as required. At leas
90% of the monthly reports are submitted
within 3 working days after receiving the
monthly activity summaries. At least 75% of
he quarterly reports are submitted by the
15th day of the month following the calendar
quarter; the grace period for a single defaul
is an additional five working days. No
annual report is delayed more than 30 days.
Project plans (for WFOs) are developed in a
least 90% of the cases within one month of
approval to start work.

3.3.1.3

Safeguards directives (orders, manuals, guides),
site security plans, and facility's security
categorization are reviewed as requested. DOE
Anomaly Review Team (DART) members are
provided. Communication with sites on
corrective actions is provided as needed.

At least 90% of the reviews meet the SO
requested due date. DART team responds to
HQ needs within 3 days of request.

Periodic
Inspection

3.3.1.4

Assistance to field offices and others in Material
Control and Accountability (MC&A), nuclear
material measurement methods and measuremen
control is reviewed at least annually.

Select and assign staff within 5 working days
of receiving request for assistance for at leas
80% of the requests. Submit 90% of the
assistance work activity work reports within
30 calendar days of the end of the assistance
activity.

100%
Inspection

New Brunswick Laboratory

Method
Customer
Feedback

Periodic
Inspection

16

U.S. Department of Energy

Draft Quality Assurance Surveillance Plan

4. Nuclear Metrology Services
The following table provides the performance standards, AQLs and surveillance methods pertaining to
Nuclear Metrology Services. Tasks include, but are not limited to:
•
•
•

Provide Traceability of Results to National and International Reference Base;
Quality Assurance (QA); and
Measurement Uncertainty.
## PWS
Section
3.4.1.1

3.4.2.1

3.4.3.1

Performance Standard

Acceptable Quality Level (AQL)

Surveillance
Method

Annually engage in joint measurement
evaluations of reference materials to compare
U.S. and other national nuclear certification
facilities
Involvement (e.g., committees, meetings,
reviews, reports, audits/inspections) in quality
systems management, QA practices, audit and
inspection practices, or QA standards is
reviewed annually.
Development and maintenance of uncertainty
calculation spreadsheets or templates tha
follow ISO guidelines are reviewed for
certifications.

Provide measurement results for at least one
inter-laboratory measurement evaluation
program per year for actinide elements within
60 days of the program results due date.

100%
Inspection

Demonstrate involvement by at least one NBL
staff member in at least one area (e.g.,
committees, meetings, reviews, reports,
audits/inspections) per year.

100%
Inspection

Demonstrated use of uncertainty spreadsheets
or templates that follow ISO guideline for
CRMs for the major isotopic abundances and
major elemental quantities certified.

Periodic
Inspection

5. Measurement Services
The following table provides the performance standards, AQLs and surveillance methods pertaining to
Measurement Services. Tasks include, but are not limited to:
•
•
•
•
•
•
•
•
•

Administrative Measurement Services Activities;
Sampling, Chemical Preparation, and Separation of Materials;
Instrument and Analytical Equipment Maintenance and Upgrades;
Elemental Analysis;
Isotopic Abundances;
Impurities and Trace Element Analysis;
Other Analyses;
Nuclear Low-level Environmental Safeguards Analysis; and
Conversion and Analysis of UF 6

New Brunswick Laboratory

17

U.S. Department of Energy

Draft Quality Assurance Surveillance Plan

PWS
Section

Performance Standard

Acceptable Quality Level (AQL)

Surveillance
Method

3.5.1.3

Demonstrated use of reference materials
as comparator/quality control samples for
reported measurements is reviewed semiannually.

100% use of reference materials as quality controls in
each set of reportable measurements utilizing
approved procedures in making the measurements.
Any software used for calculations associated with the
measurements have 100% appropriate quality controls
in place.

Random
Sampling

100% of certifications, verifications and reported
samples are prepared using qualified/approved
procedures.

Random
Sampling

100% of performance and log books (paper or digital)
on instrument maintenance and repairs are performed
o assure the least amount of equipment inoperability
ime.

Random
Sampling

3.5.2.2

3.5.3.1

Demonstrated use of qualified or
approved procedures for preparing a
range of sample types is reviewed
annually.
Demonstrated instrument repair, required
maintenance records, tracking of
associated time for repair and
maintenance, and notations on poor
instrumental performance with associated
problem resolution are reviewed annually.

3.5.3.4

Demonstrated semi-annual balance
service and certification.

3.5.4

Review annually the demonstrated
achievement of, or approach to, state-ofthe-art elemental measurement capability
for the identified methods.

3.5.5

Review annually demonstrated
achievement of, or approach to, state-ofthe-art isotopic measurement capability in
identified methods.

3.5.6

Review annually demonstrated
achievement of, or approach to, state-ofthe-art impurities and trace elemen
measurement capability.

3.5.7

3.5.8

3.5.9

Review annually demonstrated capability
o screen samples and provide
quantitative gamma-ray measurements of
americium.
Review annually demonstrated capability
o measure nuclear, low-level
environmental or comparable samples
Review annually the UF6 conversion
apparatus capability of the facility.

New Brunswick Laboratory

100% compliance with semi-annual balance service
and certification requirements as evidenced in the
service record and data on the associated master
weights, secondary weights and quality control
reports.
100% Compliance that for a pure uranium reference
material the repeatability of five high precision
itration measurements (1 sigma) is less than 0.04%
and that for a pure plutonium reference material the
repeatability of five coulometry measurements (1
sigma) is less than 0.1%.
100% compliance that for a pure uranium reference
material with a 235U/238U ratio between 0.1 and 1,
repeatability of ten accepted TIMS or UF6
measurements (1 sigma) is below 0.05% and that for a
pure plutonium reference material with a
240Pu/239Pu ratio between 0.1 and 1, repeatability of
en accepted TIMS measurements (1 sigma) is below
0.05%.
100% capability to detect uranium in a simple matrix
at or below 10 pg/g. Validate that an acceptable plan
is in place and being followed to achieve the
demonstrated capability to measure 60% of the
ransition plus the rare earth elements by December
2008.
100% facility capability to measure americium in a
plutonium sample or calibration standard to within 2%
(1 sigma) uncertainty for Am contents greater than 2%
.
100% facility capability to measure amounts of >100
pg but less than 1 ng of Pu and >0.3 ng but less than 3
ng of U with uncertainties of 3% or better.
The apparatus shall be capable of converting uranium
oxides to UF6 and providing results from UF6 or
TIMS measurements that agree to within 0.1% (2
sigma)

Random
Sampling

Periodic
Inspection

Periodic
Inspection

Periodic
Inspection

Periodic
Inspection
Periodic
Inspection
Periodic
Inspection

18

U.S. Department of Energy

Draft Quality Assurance Surveillance Plan

6. Compliance with Nuclear Analytical Laboratory Operational Requirements
The following table provides the performance standards, AQLs and surveillance methods pertaining to
Compliance with Nuclear Analytical Laboratory Operational Requirements. Furthermore, this portion of
he PRS describes the standards by which the SP shall meet the task assignments. Tasks include, but are
not limited to:
•
•
•
•
•

Environmental Safety and Health;
Safeguards and Security Activities;
Quality Assurance and Control;
Statistical Analysis of Measurement Uncertainty; and
Packaging and Shipping.
## PWS
Section

Performance Standard

3.6

Review follow-up actions for reviews,
audits and inspections of operational
requirements to meet reporting schedules.

3.6.1.1.6

Review program to monitor and control
radiation in the workplace in accordance
with 10CFR 835 (Occupational Radiation
Protection) annually.

3.6.1.2.3

3.6.1.7.2

3.6.1.9.1

All existing and new projects and activities
will be reviewed to determine if they are
bounded by the facility safety envelope.
The process used shall be in accordance
with 10 CFR 830.203, Unreviewed Safety
Question Process.
Safety Significant Systems and the facility's
Documented Safety Analysis are reviewed
annually for compliance. All nonconformance issues, reported in accordance
with DOE O 231.1A, Environment, Safety
and Health Reporting, are reviewed
monthly.
Unreviewed safety questions for all
chemical and safety hazards, radiation
protection and monitoring, safety system
requirements, and waste disposal paths
identified for any new or ongoing projec
are reviewed when identified and before
starting work.

New Brunswick Laboratory

Acceptable Quality Level (AQL)
All external reviews, audits and inspections required
o monitor the compliance of facility operations
with applicable statutes, regulations, orders policies
and guidelines are supported at the request of the
DGR. At least 90% of the follow-up actions mee
he agreed-to schedules, or the DGR has agreed, in
writing, to an alternate schedule.
Facility exposure goals are met 95% of the time.
Any instance of exposure above the facility goal is
reported to the DGR and reviewed by the SP to
determine if processes or procedures should be
changed to assure that the cause does not result in
additional exposure. Lost/Restricted Work Day
case rates are below the mean for Standard
Industrial Classification (SIC) system code 8734,
Testing Laboratories.
95% of ongoing and new projects and activities
have been screened. Should an unscreened ongoing
or new project or activity be identified, the failure to
screen the project or activity is identified and
reported as required, and the project or activity is
hen screened.

Surveillance
Method

Periodic
Inspection

Periodic
Inspection

Periodic
Inspection

Safety significant systems are operated/maintained
in accordance with DSA 95% of the time. Nonconformances are reported within the time limits
specified in DOE M 231.1-2 90% of the time.

Periodic
Inspection

All required reviews have been completed before
any laboratory operation begins for 95% of the
ongoing or new projects or activities. Any
laboratory operation that is begun before all
required reviews have been completed is stopped,
reported as required in DOE M 231.1-2 90% of the
ime, and the review is completed before laboratory
operations continue on the project or activity.

100%
Inspection

19

U.S. Department of Energy
## PWS
Section

Performance Standard

3.6.2.1.4

Review the NBL accounting ledger of all
external transactions involving nuclear
material (reportable and non-reportable)
annually.

3.6.2.1.11

Export controls meet DOE requirements.

3.6.2.2.5

Receipt and handling of classified
information in accordance with DOE O
471.2A (Information Security Program) or
its successor(s) is reviewed annually.
Incidents of concern are reported per DOE
O 471.4 (Incidents of Security Concern) or
its successor(s).

3.6.2.3.2

Cyber security risks management to meet
DOE requirements is reviewed annually.

3.6.2.4.5

Information regarding foreign visits to
NBL, entered into FACTS or its
replacement, is reviewed prior to the visit.
Recordkeeping for DOE O 142.1
(Classified Visits Involving Foreign
Nationals) and DOE O 142.3 (Unclassified
Foreign Visits and Assignments) or their
successors, is reviewed annually.

3.6.2.4.6

3.6.2.6.6

The facility security program to meet
requirements of DOE O 471.1, Change 1
(or its successor(s)), is reviewed annually.
The facility self assessment and audi
program is reviewed annually. Attend all
close-out meetings of senior managemen
personnel where findings, observations and
recommendations are presented.

3.6.3.1

Review the quality management system
which includes document control, training
qualification assurance, corrective action
racking and records management annually.

3.6.3.5

The deficiencies tracking system is
reviewed, from identification to closure,
annually.

New Brunswick Laboratory

Draft Quality Assurance Surveillance Plan
Acceptable Quality Level (AQL)
At least 95% of transactions are logged on the same
day as the transaction. When a transaction canno
be logged on the same day as the transaction, a
reason is identified in the ledger. When a
ransaction has not been logged on the same day and
he fact is identified, the transaction is logged with
he date of the log-in and the date of the transaction
is also noted in the ledger.
The sensitive subjects list provided contains at leas
90% of the sensitive subjects covered by the
laboratory. A sample of at least one document of
each type requiring export control approval is
reviewed and found to have the appropriate
authorizations.
At least 95% of the classified material is handled in
accordance with requirements. Non-conformances
are reported within the time limits specified in DOE
0 471.4 90% of the time.
Cyber security risks are assessed and mitigated or
submitted to the DGR for acceptance and approved
before new software or hardware are installed or
cyber systems are exposed to the risk; cyber security
documentation updates are provided within 30 days
of the requested due date.
All foreign visitors are logged into FACTS before
coming on site. At least 90% of visitors from
sensitive counties are logged into FACTS at least 45
days in advance of the beginning of the visit. A
least 90% of visitors from non-sensitive countries
are logged into FACTS at least 30 days in advance
of visit. At least 80% of visits are closed out in
FACTS by 90 days after the visit has ended.
Records accurately reflect foreign visitor
documentation as required in DOE O142.1.
Facility security by external and internal reviews is
satisfactory as defined in DOE O 471.1, Change 1.
SP self assessments and audits are completed within
2 months; a written report is provided within one
week of close out meeting. The SP reviews
corrective actions within two weeks of completing
heir completion.
For 90% of the reviewed plans and operating
procedures, the approvals were documented and in
place. For 90% of the reviewed training, the
required training was performed within 15 working
days of the due date, or the authority of the person
o perform the function was removed. For 90% of
he completed corrective actions reviewed,
responses were documented and closed. For 90% of
he records reviewed, the records were accurately
filed.
A statistical sampling of the records for quality
deficiencies is reviewed; 90% of the QA
deficiencies reviewed have documentation which
accurately reflects the status at the time of the
review.

Surveillance
Method

Random
Sampling

Random
Sampling

Random
Sampling

Periodic
Inspection

Periodic
Inspection

Periodic
Inspection

Random
Sampling

Random
Sampling

Random
Sampling

20

U.S. Department of Energy

Draft Quality Assurance Surveillance Plan

PWS
Section

Performance Standard

Acceptable Quality Level (AQL)

3.6.3.9

Review of demonstrated quality control of
measurements and instrument calibrations
with statistical analysis of quality control
samples over time for trend analysis is
performed annually.

3.6.3.10

Review facility documentation annually to
determine if established review cycles exis
and if the review cycle meets DOE
requirements.

At least 90% of the QC measurements (elemental
and isotopic) for techniques with a sufficien
number of QCs for evaluation were reviewed and
documented at least annually for trends in reported
results.
A statistical sampling of facility plans and analytical
procedures is reviewed; 90% of the documents
reviewed have an established review cycle which
meets DOE requirements and of those, 90% have
been reviewed or revised as required within 30 days
of the required date.
At least 90% of training requirements are completed
within 15 working days after the required training or
re-training date. Findings associated with training
and qualification of NBL personnel which are
discovered during self-assessments have solutions
identified which are incorporated into the training
plan within 90 days.
A statistical report or description of statistical
methods and results is provided for each new
qualified techniques and each reference material
certified. ISO and ANSI guidelines are met for
uncertainty budget calculations.
At least 95% of the paperwork prepared for
shipping hazardous materials is compliant with
regulations. When errors in paperwork are
identified, notifications are made to the DGR and to
others as required by shipping authorities. The
errors are reviewed to determine if processes or
procedures need to be changed to assure that the
same error does not occur again.
Document reviews are completed within 30 days of
he due date which meets DOE requirements, or
approval for delay of the review or revision will be
obtained in writing from the DGR. Emergency
exercises are held in collaboration with the ANL
site Emergency Operations Center as required.
There are no incidents of failure to cooperate with
site emergency personnel.

3.6.3.15

3.6.4

Annually review the training and
qualifications program developed in
accordance with DOE O 5480.20A Change 1 (Personnel Selection,
Qualification, and Training Requirements
for DOE Nuclear Facilities) or its
successor(s).
Annually review the capability to perform
statistical analysis of data for qualification
of new analytical techniques and whether
or not ISO and ANSI guidelines are met for
uncertainty budget calculations.

3.6.5.7

Annually review shipping documentation
for all hazardous materials; these should
meet 49 CFR and IATA requirements.

3.6.6.3

Annually review Emergency Plans and
implementing procedures to assure tha
planning and resources are adequate to
meet DOE requirements, maintained and
exercised.

Surveillance
Method
100%
Inspection

Random
Sampling

Random
Sampling

100%
Inspection

Random
Sampling

Periodic
Inspection

7. Measurement Development
The following table provides the performance standards, AQLs and surveillance methods pertaining to
Measurement Development. Tasks include, but are not limited to:
•
Reference Material Production and Certification Methods;
•
Chemical Analysis and Separation Methods;
•
Mass Spectrometry, Impurities, and Trace Element Analysis Methods;
•
Cleanroom and Low-level Environmental Chemical Processing Techniques;
•
NDA Techniques;
•
Capital Equipment Improvements;
•
Publications; and
•
Meetings with the Scientific Community.

New Brunswick Laboratory

21

U.S. Department of Energy

Draft Quality Assurance Surveillance Plan

PWS
Section

Performance Standard

Acceptable Quality Level (AQL)

3.7.1.4

Annually review the plans and processes for
producing new, synthetic calibration materials
for isotopic CRMs which are traceable to the
national reference base.

3.7.2.2

Annually review efforts to expand or improve
elemental measurement capabilities.

3.7.2.3

Annually review efforts to expand or improve
chemical separation capabilities.

3.7.3.3

Annually review efforts to expand or improve
ICPMS capabilities

3.7.6

Review annual recommendations for capital
equipment replacements or improvements.

Within one year, prepare plans, documents, or
reports that describe the production or use of
new synthetic calibration materials for isotopic
CRMs.
Within one year, prepare plans, documents, or
reports that describe the development or use of
at least one new elemental analysis technique.
Within one year prepare plans, documents, or
reports that describe the development or use of
at least one new chemical separation technique.
Within one year prepare plans, documents, or
reports that describe the development or use of
new ICPMS techniques.
Provide or maintain an annual list of capital
equipment needs that details the type of
equipment, specifications, approximate cost,
and estimated associated costs for installation
(including building modification, factory
installment, etc.).

Surveillance
Method
100%
Inspection
100%
Inspection
100%
Inspection
100%
Inspection

100%
Inspection

8. Serve on Consensus Standards-Writing Committees and Working Groups
The following table provides the performance standards, AQLs and surveillance methods pertaining to
Serving on Consensus Standards-Writing Committees and Working Groups Tasks include, but are no
limited to:
•
Service on Consensus Standards; and
•
Service on Government Working Groups.
## PWS
Section

Performance Standard

Acceptable Quality Level (AQL)

Surveillance
Method

3.8

Annually review efforts to write or support
hose writing consensus standards for the
nuclear industry.

At least two persons participate at least
annually as subject matter experts on panels,
committees or subcommittees writing
consensus standards for the nuclear industry.

Periodic
Inspection

9. Laboratory Administration
The following table provides the performance standards, AQLs and surveillance methods pertaining to
Serving on Consensus Standards-Writing Committees and Working Groups Tasks include, but are no
limited to:
•
•
•
•
•
•
•
•
•
•
•

Finance and Budget;
Timekeeping and Accounting for NBL Programs, Projects, and Activities;
Records Management;
Computer Systems Management;
Web Page Management;
Travel Processing and Coordination;
Activity Status Reports;
Property Management;
Procurement;
Public Relations and Marketing;
Stockroom; and

New Brunswick Laboratory

22

U.S. Department of Energy
•

Draft Quality Assurance Surveillance Plan

Administrative Support Activities.
## PWS
Section

Performance Standard

3.9.1.2

Annually review financial and other resource
records for adequacy.

3.9.3.1

Annually review document filing and storage
for adequacy.

3.9.8

Annually review property records for
adequacy.

New Brunswick Laboratory

Acceptable Quality Level (AQL)
Financial and other resource records requested
by the DGR are available 90% of the time within
he agreed to deadline.
A statistical sample of documents prepared
during the year are selected for review. 90% of
he documents reviewed were properly filed in
he central filing system.
The status of a random selection of property
items from each category is evaluated based on
he last inventory. At least 90% of the items
reviewed were appropriately accounted for in the
most recent inventory of that category of items.

Surveillance
Method
Periodic
Inspection
Random
Sampling

Random
Sampling

23

U.S. Department of Energy

Draft Quality Assurance Surveillance Plan

APPENDIX B:

SERVICE PROVIDER DISCREPANCY REPORT

SERVICE PROVIDER DISCREPENCY REPORT
2. TO: (Service Provider and Manager Name)

3. FROM: (Name of DGR)

PREPARED

DATES
## RETURNED BY CONTRACTOR

ORAL NOTIFICATION

1. DISCREPENCY REPORT
## NUMBER

ACTION COMPLETE

4. DISCREPENCY OR PROBLEM (Describe in Detail. Include PWS references. Attach Continuation Sheet if Necessary.)

5. SIGNATURE OF DGR

6. TO: (Name of DGR)

7. FROM: (Service Provider)

8. SERVICE PROVIDER RESPONSE AS TO CAUSE, EFFECT, CORRECTIVE ACTION AND ACTIONS TO PREVENT
RECURRENCE. (Attach Continuation Sheet if necessary. Cite applicable SP QC program procedures or new QC procedures.)

9. SIGNATURE OF SP REPRESENTIVE

10. DATE

11. GOVERNMENT EVALUATION (Acceptance, partial acceptance, or rejection. Attach Coordination Sheet if necessary.)

12. GOVERNMENT ACTIONS (Cure notice, show cause, other.)

CLOSE OUT
## NAME AND TITLE

SIGNATURE

DATE

SP NOTIFIED
## QAE
## DGR

New Brunswick Laboratory

24

U.S. Department of Energy

APPENDIX C:

Draft Quality Assurance Surveillance Plan

CUSTOMER FEEDBACK RECORD
## CUSTOMER FEEDBACK RECORD

DATE AND TIME OF COMPLAINT
## SOURCE OF COMPLAINT

ORGANIZATION
## INDIVIDUAL

NATURE OF COMPLAINT

PWS REFERENCE
## VALIDATION

DATE AND TIME SERVICE PROVIDER INFORMED OF COMPLAINT

NAME OF SP REPRESENTATIVE INFORMED OF COMPLAINT

ACTION TAKEN BY SERVICE PROVIDER (Responsible officer)

RECEIVED AND VALIDATED BY

Determination:

Complaint Valid 

New Brunswick Laboratory

Complaint Invalid 

25

U.S. Department of Energy

APPENDIX D:

Draft Quality Assurance Surveillance Plan

SAMPLING GUIDE/INSPECTION CHECKLIST

SERVICE FUNCTION: ________________________________________________________
PWS SECTION: ________________________________________________________
NOTE: E = Excellent Performance
N/A = Not Applicable

S = Satisfactory Performance

U = Unsatisfactory Performance

1
2
3
4

Method of Surveillance:
Lot Size:
Sample Size:
Performance Requirement: Performance is excellent (E) when _____ or fewer defects are
discovered per month. Performance is satisfactory (S) when _____ or fewer defects are
discovered per month. Performance is unsatisfactory (U) when ________________ or more
defects are discovered per month.

5

Sampling Procedure: Instructions on how to select the sample must be clear and complete

6

Inspection Procedure: The procedure must be detailed enough to allow a yes/no objective
decision as to the acceptability of performance by anyone making the inspection. Explain when
evaluation is to occur and what is acceptable/unacceptable
Performance: Excellent (E), Satisfactory (S),
Unsatisfactory (U), Not Applicable (N/A)
PRS Requirements

Timeliness

Quality of Work

Notes

.

Overall Rating Of Inspection (E, S,
U, or N/A)
Inspector Comments:
SP Signature:
QAE Signature:

New Brunswick Laboratory

Date: _________________
Date:

26

U.S. Department of Energy

APPENDIX E:
## ANSI
## AQL
## CO
## COR
## CRM
## DART
## DGR
## DOE
## HQ
## ICPMS
MC&A
## ME
## MEO
## NBL
## NSNS
## OMB
## PRS
## PWS
## QA
## QAE
## QASP
## QC
## QCP
## SME
## SO
## SP
## TE
## WFO

Draft Quality Assurance Surveillance Plan

ACRONYMS
American National Standards Institute
Acceptable Quality Level
Contracting Officer
Contracting Officer Representative
Certified Reference Material
DOE Anomaly Response Team
Designated Government Representative
U.S. Department of Energy
Headquarters
Inductively Coupled Plasma-Mass Spectrometry
Material Control and Accountability
Measurement Evaluation
Most Efficient Organization
New Brunswick Laboratory, Argonne, IL
Nuclear Safeguards and Noproliferation Support, Program at NBL
Office of Management and Budge
Performance Requirements Summary
Performance Work Statemen
Quality Assurance
Quality Assurance Evaluator
Quality Assurance Surveillance Plan
Quality Control
Quality Control Plan
Safeguards Measurement Evaluation, Program at NBL
Office of Security
Service Provider
Technical Exhibi
Work for others

New Brunswick Laboratory

27


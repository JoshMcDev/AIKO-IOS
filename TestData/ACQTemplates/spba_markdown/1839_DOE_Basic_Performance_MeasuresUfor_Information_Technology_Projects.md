# 1839_DOE_Basic_Performance_Measures_for_Information_Technology_Projects

_Converted from PDF using pdftotext_

PE-WI-V3-011502

Department of Energy
(DOE)

Basic Performance Measures for
Information Technology Projects

Guidance White Paper

January 15, 2002

Office of the Chief Information Officer
Office of the Associate CIO for Architecture, Standards, and Planning
Software Quality and Systems Engineering Program

Basic Performance Measures For Information Technology Projects
Guidance White Paper
Introduction
The purpose of this "white paper" is to provide guidance for project teams in the identification of
performance measures oriented to information technology (IT) projects, which allow for the
collection and reporting of project data to help track and assess project progress, product quality,
project success, and customer satisfaction.
An effective set of performance measures will provide actionable information, on a focused se
of metrics, to provide a balanced view of project performance that can be used to make decisions
o improve the project management process. Performance measures also provide for
accountability by laying out what is expected, when it is expected, and what action will be taken
if planned achievements do not occur.
Terms Used in This Paper
Performance. The execution or accomplishment of work.
Measure. A measure is the result of the activity involved in determining dimension, i.e., size,
etc. through measuring. Measures should be objective, timely, simple, accurate, useful, and costeffective.
Metric. The Institute of Electrical and Electronics Engineers (IEEE) defines metric as a
quantitative measure of the degree to which a system, component, or process possesses a given
attribute.
Indicator. An indicator is a metric or combination of metrics that provide insight into a process,
a project, or a product, to enable assessment and improvement.
Following is the typical sequence of performance measurements related events for an IT project:
performance measures are identified and documented Æ measurement data is collected Æ
metrics are developed Æ indicators are obtained.
Performance Measures
The following definition is from the Performance-Based Special Interest Group, PerformanceBased Management Handbook, Volume 2, Establishing an Integrated Performance Measuremen
System, developed for DOE.
“Performance Measurement is the ongoing monitoring and reporting of program
accomplishments, particularly progress towards pre-established goals. It is typically conducted
by program or agency management. Performance measures may address the type or level of
program activities conducted (process), the direct products and services delivered by a program
(outputs), and or the results of those products and services (outcomes). A “program” may be any
activity, project, function, or policy that has an identifiable purpose or set of objectives.”
DOE Software Quality and
Systems Engineering
PE-WI-V3-011502.doc

Page 2 of 8

January 15, 2002

Basic Performance Measures For Information Technology Projects
Guidance White Paper
OMB Requiremen
OMB Circular A-130 indicates that, as part of an agency’s Capital Planning and Investmen
Process, it must institute performance measures and management processes that monitor actual
performance to expected results. Measurements can be reported at the program and project level
and include resource and cost goals, schedule and progress goals, trade-offs and risk outcomes,
product quality goals, and customer satisfaction goals.
Performance Measures - Basic Categories
Measures of efforts. Efforts are the amount of resources, in terms of money, people, materials,
etc., applied to a program or project.
Examples: The amount of money spent and the number of person-hours burned on a project.
Measures of accomplishments. Accomplishments are milestones achieved with the resources
used. There are two types of accomplishments - outputs and outcomes. Outputs relate to the
quantity of goods or services produced; outcomes relate to the results of providing those outputs.
Examples - Output: Number of modules coded, number tested, number inspected.
Example - Outcome: Gross salary function of new payroll system completed on schedule.
Measures that relate efforts to accomplishments. These measures are associated with resources
or cost relative to accomplishments achieved. They provide information about the production of
an output at a given level of resource use and demonstrate an entity’s capability when compared
with previous results, internally established goals and objectives, generally accepted norms or
standards, or results achieved by similar entities.
Example: Amount of money expended for the portion of project completed versus the amount of
money planned to be expended for the portion of work planned at a set point during the projec
(e.g., earned value).
Performance Measures - Key Objectives
•
•
•
•
•

Assess project status.
Develop early warning indicators.
Monitor product quality.
Manage schedule, budget, and scope.
Track the project's alignment with business goals

DOE Software Quality and
Systems Engineering
PE-WI-V3-011502.doc

Page 3 of 8

January 15, 2002

Basic Performance Measures For Information Technology Projects
Guidance White Paper
Metrics vs. Requirements
Occasionally, a misunderstanding may arise stemming from the use of the word metrics and
requirements interchangeably. Metrics, and indicators derived from them, are typically used by
project managers and project teams to assess project status and track project progress.
Requirements are the wants and needs of the project's customer(s) and/or the product's users.
Requirements are "met" or "satisfied" by the product's functional content or performance under
use. Note, however, that a requirement may in fact provide a metric to align the project’s
progress with the product’s quality or business goal. The following is an example:
Metric: Reduce number of duplicate data elements by 20 percent.
Requirement: The system shall capture all data elements and store them in a single database (to
reduce/eliminate duplication).
In this example, satisfying the requirement also provides the measurement for developing and
reporting the metric.

Types of Performance Measures
Process Metrics

Increase capability level (i.e., SEI-CMM levels)
Do more with less (shorter schedule, less resources)
Improve quality (less defects, less re-work)
Track project progress
Assess project status
Award contract fees
Determine product quality
Identify defect rates
Ensure product performance

Project Metrics

Product Metrics

Typical Metric Categories
Schedule
Budge
Functionality

Actual vs. planned:
- Schedule and progress
Actual vs. planned:
- Resources and cos
Delivered vs. planned:
- Product characteristics
- Technology effectiveness
- Process performance
- Customer satisfaction

DOE Software Quality and
Systems Engineering
PE-WI-V3-011502.doc

Page 4 of 8

January 15, 2002

Basic Performance Measures For Information Technology Projects
Guidance White Paper
Measures vs. Indicators - Example: Finding defects in products (e.g. a Requirements
Document)
Basic Measures:
- No. of requirements reviewed
- No. of reviewers involved
- No. of defects found
- Effort expended

Indicators: Efficiency
- No. reviewed/effor
- No. reviewed/time
- No. found per effort, time
Indicators: Effectiveness:
- % found of those expected
- % escaped

Examples of Performance Measures
The following table provides examples of performance measures that are typical for many IT
projects. While the Category and Metrics columns are fairly representative of those used in IT
projects in general, the Measure of Success will vary greatly and should be established for each
individual project, as appropriate.

Category

Schedule
performance

Focus
Tasks completed vs. tasks
planned at a point in time.
Major milestones met vs.
planned.
Revisions to approved plan.
Changes to customer
requirements.
Project completion date.

Revisions to cost estimates.

Budget
performance

Dollars spent vs. dollars
budgeted.
Return on investment (ROI).
Acquisition cost control.

Defects identified through
quality activities.

Product
Quality

Test case failures vs. number of
cases planned.

DOE Software Quality and
Systems Engineering
PE-WI-V3-011502.doc

Purpose

Measure of Success

Assess project progress.
Apply project resources.
Measure time efficiency.

100% completion of tasks on
critical path; 90% all others
90% of major milestones met.

Understand and control project
"churn."
Understand and manage scope
and schedule.
Award / penalize (depending
on contract type).

All revisions reviewed and
approved.
All changes managed through
approved change process.
Project completed on schedule
(per approved plan).

Assess and manage project
cost.
Measure cost efficiency.

100% of revisions are reviewed
and approved.
Project completed within
approved cost parameters.
Track and assess performance ROI (positive cash flow) begins
of project investment portfolio. according to plan.
Assess and manage acquisition All applicable acquisition
dollars.
guidelines followed.
Track progress in, and
effectiveness of, defec
removal.
Assess product functionality
and absence of defects.

Page 5 of 8

90% of expected defects
identified (e.g., via peer
reviews, inspections).
100% of planned test cases
execute successfully.
January 15, 2002

Basic Performance Measures For Information Technology Projects
Guidance White Paper
Category

Focus
Number of service calls.

Track customer problems.

Customer satisfaction index.
Customer satisfaction trend.
Number of repeat customers.

Identify trends.
Improve customer satisfaction.
Determine if customers are
using the product multiple
imes (could indicate
satisfaction with the product).
Assess quality of projec
deliverables.

Number of problems reported by
customers.

Compliance

Redundancy

Cost
Avoidance

Customer
Satisfaction

Purpose

Measure of Success
75% reduction after three
months of operation.
95% positive rating.
5% improvement each quarter.
“X”% of customers use the
product “X” times during a
specified time period.
100% of reported problems
addressed within 72 hours.

Compliance with DOE
Enterprise Architecture model
requirements.
Compliance with Interoperability
requirements.
Compliance with DOE
standards.
For web site projects,
compliance with Style Guide.
Compliance with Section 508.

Track progress towards
Department-wide architecture
model.
Track progress towards system
interoperability.
Alignment, interoperability,
consistency.
To ensure standardization of
web site.
To meet regulatory
requirements.

Zero deviations without proper
approvals.

Elimination of duplicate or
overlapping DOE systems.

Ensure return on investment.

Decreased number of duplicate
data elements.
Consolidate help desk functions.

Reduce input redundancy and
increase data integrity.
Reduce $ spent on help desk
support.

Retirement of 100% of
identified systems (as
committed in SIM or ABC)
Data elements are entered once
and stored in one database.
Approved consolidation plan by
June 30, 2002.

System is easily upgraded.

Take advantage of e.g., COTS
upgrades.

Avoid costs of maintaining
duplicate systems.
System is maintainable.

Reduce IT costs.

System availability (up time).

Measure system availability.

System functionality (meets
customer's / user's needs).
Absence of defects (that impac
customer).

Measure how well customer
needs are being met.
Number of defects removed
during project lifecycle.

DOE Software Quality and
Systems Engineering
PE-WI-V3-011502.doc

Reduce maintenance costs.

Page 6 of 8

Product works effectively
within system portfolio.
No significant negative findings
during architect assessments.
All web sites have the same
“look and feel.”
Persons with disabilities may
access and utilize the
functionality of the system.

Subsequent releases do not
require major “glue code”
project to upgrade.
100% of duplicate systems have
been identified and eliminated.
New version (of COTS) does
not require “glue code.”
100% of requirement is met.
(e.g., 99% M-F, 8am to 6pm,
and 90% S & S, 8am to 5pm).
Positive trend in customer
satisfaction survey(s).
90% of defects expected were
removed.
January 15, 2002

Basic Performance Measures For Information Technology Projects
Guidance White Paper
Category

Focus
Ease of learning and use.

Productivity

Measure of Success

Measure time to becoming
productive.
Manage/reduce response
imes.
Assess effectiveness and
quality of training.

Positive trend in training
survey(s).
95% of severity one calls
answered within 3 hours.
90% of responses of "good" or
better.

Validate system supports
program mission
Improve customer satisfaction
and national interests.
Track reduction of costs to
maintain system.
Reduce costs associated with
upgrading user's systems.

All reportable inventory is
racked in system.
Improve turnaround time from
2 days to 4 hours.
Reduce maintenance costs by
2/3 over 3-year period.
Reduce upgrade costs by 40%.

Time taken to complete tasks.

To evaluate estimates.

Number of deliverables
produced.

Assess capability to deliver
products.

Completions are within 90% of
estimates.
Improve product delivery 10%
in each of the next 3 years.

Time it takes to answer calls for
help.
Rating of training course.

Business
Goals/
Mission

Purpose

Functionality tracks reportable
inventory.
Turnaround time in responding
o Congressional queries.
Maintenance costs.
Standard desktop platform.

The following set of questions is intended to assist in stimulating the thought process to
determine performance measures that are appropriate for a given project or organization.
Project / Process Measurement Questions
•
•
•
•
•

What options are available if the schedule is accelerated by four months to meet a tight
market window?
How many people must be added to get two months of schedule compression and how much
will it cost?
How many defects are still in the product and when will it be good enough so that I can ship
a reliable product and have satisfied customers?
How much impact does requirements growth have on schedule, cost and reliability?
Is the current forecast consistent with our company's historical performance?

Organizational Measurement Questions
•
•
•
•
•

What is the current typical cycle time and cost of our organization's development process?
What is the quality of the products our organization produces?
Is our organization's development process getting more or less effective and efficient?
How does our organization stack up against the competition?
How does our organization's investment in process improvement compare with the benefits
we have achieved?

DOE Software Quality and
Systems Engineering
PE-WI-V3-011502.doc

Page 7 of 8

January 15, 2002

Basic Performance Measures For Information Technology Projects
Guidance White Paper
•
•

What impact are environmental factors such as requirements volatility and staff turnover
having on our process productivity?
What level of process productivity should we assume for our next development project?

References
•

Practical Software and Systems Measurement http://psmsc.com

•

MIT Information Systems http://web.mit.edu/is/projects/metrics.html

•

"Measuring Up", CIO Magazine, September 15, 1999

•

DOE Strategic Plan, I/S Initiatives http://www.hr.doe.gov/imspgol2.html

•

Balanced Team Scorecard, DRM Associates http://www.npd-solutions.com/scorecard.html

•

Product Development Metrics List, DRM Associates
http://www.npd-solutions.com/scorecard.html

•

BMIS-Phoenix Project Measures http://crinfo.doe.gov/bmis/FocusDocs/focus_docs.htm

•

How to Measure Performance, DOE, DP Special Projects Group
http://www.orau.gov/pbm/handbook/

•

INCOSE 2001 Seminar on Metrics, Hampton, VA

•

Guidelines for Software Measurement, DOE Nuclear Weapons Complex Software Quality
Assurance Subcommittee (SQAS) http://cio.doe.gov/sqas/publications.htm

•

ROI and the Value Puzzle, Capital Planning and IT Investment Committee, Federal CIO
Council, April 1999

DOE Software Quality and
Systems Engineering
PE-WI-V3-011502.doc

Page 8 of 8

January 15, 2002


# 1843_Implementing_Best_Practices_Strategies_at_Work

_Converted from PDF using pdftotext_

e

June 1998

ittee

Implementing•
Best•
Practices•
•
Strategies at Work

mm

Capital

tm

Co

Pl

ni

ng

IT Inve
d
n
a
s

nt

an

Federal CIO Council

Table of Contents
Executive Summary...........................................................1
Background.............................................................................................9
General Observations................................................15
Best Practices...................................................................................21
¥ Case Studies
Ð General Services Administration (GSA)
Ð National Aeronautics and Space
Administration (NASA)
Ð Department of Defense (DoD)
¥ Additional Agency/Organization Profiles:
Pilot Agencies
Ð Department of Agriculture
Ð Department of Energy
Ð Department of Housing and Urban
Developmen
Ð Department of State
Ð Environmental Protection Agency
Ð United States Coast Guard
¥ Additional Agency/Organization Profiles:
Other Agencies/Organizations
Ð Department of Commerce
Ð Nuclear Regulatory Commission

Appendices
A-1
¥ Appendix B: List of Acronyms.....................................B-1
¥ Appendix A: List of Attendees...................................

Page iii

PREFACE
In July 1997 twenty-four different Federal agencies and organizations gathered in
Washington for a Best Practices Workshop. Workshop participants highlighted their
approaches for selecting, controlling, and evaluating critical IT investments with
hree agencies, GSA, NASA and DoD providing extensive case studies that are
included in this document. Eight other organizations provided important details
from their Capital Planning Pilots projects, each focusing on a different aspect of the
Planning Model.
We designed this report to reflect the best practices of these pioneering Federal
organizations. All across the government CIOs and IT organizations are applying
lessons learned from the private sector and each other to screen and select projects.
These projects are monitored closely throughout their life-cycle for accountability
and verifiable return on each investment.
The Federal CIO Council would like to thank all of the agencies who took part in
he creation of this document. Without your early efforts in Capital Planning and
he commitment of your senior leadership, we would have lacked an importan
blueprint for success. Today, with flexible processes that stress strong management,
communication and a commitment to mission-driven investments, the Federal
Community is bringing the best technology to work for the American public.

Shereen G. Remez
Co-Chair, Capital Planning & IT Investment Committee

Page v

Strategies at Work

The Pilot Agency Presentations Reflected Progress
Agencies in Implementing Best Practices Within
he Select Phase and the Need to Continue
o Develop Strategies for Implementing Control
and Evaluate Processes
■

The agencies agreed that the Select, Control, Evaluate processes need
o be integrated and ongoing

■

The pilot agencies presented the progress they have made in implementing
heir Capital Planning Pilot Projects; each pilot focuses on one or more bes
practices within the Select, Control, Evaluate Model
¥ Department of Agriculture (USDA
¥ Department of Energy (DOE)
¥ Department of Housing and Urban Development (HUD)
¥ Department of State
¥ Environmental Protection Agency (EPA)
¥ U.S. Coast Guard (USCG)

■

Additional agencies and organizations provided inputs about best practices
hat are being implemented within their organizations:
¥ Department of Commerce
¥ Nuclear Regulatory Commission (NRC)

Page 3

Implementing Best Practices

The Select, Control, Evaluate Model Provides a High-level,
Structured Approach to the IT Capital Planning and
Investment Control Process
• Screen
• Rank
• Selec

Select
How do you know
you have selected
he best projects?

Evaluate

Control

Based on your
evaluation, did the
systems deliver
what you expected?

What are you doing
o ensure that the
he projects will
deliver the benefits
projected?

• Conduct PIRs
• Determine
Adjustments
• Apply Lessons
Learned

• Monitor
Progress
• Take Corrective
Action

Process
Information

Critical Success Factors
■

Secure commitment and involvement from key agency decision makers

■

Identify and implement repeatable, efficient, and consistent processes

■

Recognize the need for variety of system types: infrastructure,
administrative

■

Assign accountability

■

Identify interfaces with existing and new processes to ensure that multiple
requests are not made for the same information

Page 4

Strategies at Work

Participants of the Best Practices Workshop, Held
July 24-25, 1997, Provided Strategies for Implementing
Best Practices
■

Representatives of 24 different agencies and organizations attended
he workshop, including several CIOs, and built upon the work
hey accomplished during the First Practices Workshop held in February 1997

■

Participants used the Select, Control, Evaluate Model as a basis for
implementing the best practices and provided general observations abou
successes, barriers, and lessons learned in implementing effective capital
planning and IT investment processes

■

Agencies highlighted approaches to implementing best practices that reflect
he different characteristics of their agencies (e.g., mission, agency size,
and foot print)
¥ Three agencies, GSA, NASA, and DoD, presented integrated approaches
hat are highlighted as case studies in this documen
¥ Several pilot agencies provided detailed approaches to segments of the
overall Select, Control, and Evaluate Model; their respective approaches
follow the case studies
¥ Additional agencies provided inputs and insights to implementing bes
practices; these are included throughout the documen

Page 5

Implementing Best Practices

Workshop Participants Identified Several Success
Factors That Apply to All Phases of the Select, Control,
Evaluate Model
Leadership
¥ Strong leadership at the top
¥ CIO who can "sell" IT capabilities to the CEO/agency head

Mission Focus
¥ Decisions driven by a clearly defined business mission
¥ IT is seen as a mission enabler
¥ Emphasis on the contribution of the technology to the bottom line
mission; not on the technology itself
¥ Ability to answer the question "Is this the best solution for our business?"

Communication
¥ Developed partnership among CIO, CFO, and CEO
¥ Open communication among the agencyÕs top leadership
¥ Educate IT management and staff about the organizationÕs business

The Right People
¥ Decision making at the lowest appropriate level
¥ Appropriate IT investment review board members

Effective Processes
¥ Flexibility
¥ Clear, well-defined processes
¥ IT investment process tracks with organizationÕs overall budget cycle

Intregrated Processes
¥ Established ROI/Performance measurements at project inception

Supporting Infrastructure
¥ Installed corporate information Infrastructure (ideas, feedback easily
shared within organization)
¥ Established baseline of current IT assets

“Even the best processes for Capital Planning and IT
Investment cannot be fully effective withou
he commitment of senior management”
- Agency commen

Page 6

Strategies at Work
Agencies with Successful Implementation Strategies Shared
Several Common Themes
■

Catalysts for change prompted several agencies to quickly adapt IT investments
o the agency mission
¥ New senior managers who saw the need to define and narrow their
agencyÕs mission focus, and provided a roadmap that clarified
hat mission
¥ Outside intervention such as GAO, OMB, or Congress

■

Mission-driven investments, including IT, kept decision-makers focused on
achieving greater results

■

Open and honest communication among the primary leaders
in the organization (CIO, CFO, Agency Head, and business community)
led to better collaborative decision-making
¥ Most effective if CEO selects CIO (private-sector model)
¥ Strong agency head is essential

■

IT community understood business needs and could focus more readily on
end-user requirements

■

The CIO facilitated discussions across the agency and provided a strategy for
IT investments that is in concert with the agency mission

■

A flexible process provided a more logical approach to decision-making

■

A corporate information infrastructure was in place that provided the tools
and data needed to help managers make decisions

■

An integrated and ongoing approach to the Select, Control, Evaluate Model
in which decision-making factors used during the Select Phase were carried
hrough the Control and Evaluate Phases (e.g., performance measures
and parameters defined during Select are used to make course corrections
during Control)

■

Strong program management skills that maximized each IT investment

Page 7

Strategies at Work
The CIO Council Committee on Capital Planning and IT
Investment Met on July 24-25, 1997 to Share Successful
Implementation Approaches to Capital Planning and IT
Investment Best Practices and to Build Upon the First Practices
Workshop Held in February 1997
■

The First Practices Workshop, held February 3-4, 1997, brought together
representatives from the Federal CIO community to discuss approaches
for meeting the requirements of the Clinger-Cohen Act of 1996

■

Twenty-six agencies participated and used the GAO and OMB Select,
Control, Evaluate Model to identify the following series of first practices
¥ Secure senior management commitment and participation
¥ Establish an executive-level investment review board
¥ Select the right investments (using established criteria)
¥ Determine costs of present systems
¥ Address costs, benefits, and risks of planned investments
¥ Provide staff analysis to the investment review board that informs
decision making
¥ Make decisions when needed
¥ Control initiatives throughout the life cycle
¥ Evaluate results for lessons learned

■

Seven agencies reported on their plans for implementing Capital Planning
Pilot Projects
¥ Department of Agriculture
¥ Environmental Protection Agency
¥ Department of Energy
¥ General Services Administration
¥ Department of Housing and Urban Developmen
¥ U.S. Coast Guard
¥ Department of State

Workshop Participants Developed the Document“Information Technology Investment:
First Practices,” February 1997

Page 11

Implementing Best Practices

Participants in the Best Practices Workshop Shared
Successful Implementation Strategies and Lessons
Learned from Their Experiences with the First Practices
and GAO/OMB-Defined Best Practices
■

Representatives of 24 different agencies and organizations attended
he workshop, including several CIOs

■

The agencies that are participating in Capital Planning Pilot Projects briefed
he participants on their successes and lessons learned in implementing
heir pilots
¥ Department of Agriculture
¥ Department of Energy
¥ Department of Housing and Urban Developmen
¥ Department of State
¥ Environmental Protection Agency
¥ General Services Administration
¥ United States Coast Guard

■

The National Aeronautics and Space Administration also provided details
of their progress in successfully integrating their IT process into their overall
agency strategic management process

■

A representative from the General Accounting Office (GAO) discussed
GAOÕs expectations for agencies in integrating information technology within
he overall capital planning process

■

Workshop participants from several additional agencies provided inputs on
heir current implementation efforts during facilitated group discussions

Workshop Objective:
To gain an understanding of successful approaches for
implementing best practices by examining the efforts of
a diverse group of federal agencies (e.g., large versus
small; centralized versus decentralized; single, focused
mission versus broader multiple missions)

Page 12

Strategies at Work
Agencies Discussed Their Approaches to Implementing Bes
Practices Within the Select, Control, Evaluate Model
Participant discussion for implementing the best practices focused on:

■ Processes: the activities required to gather the information needed to
make IT investment decisions

■ People: the committees, boards, and individuals involved in the decisionmaking process

■ Tools and Techniques: the automated and systematic approaches
agencies are using to assist in decision-making

■ Success Factors: the most important ingredients that led to successful
implementation of the process

■ Challenges: the roadblocks agencies are facing in achieving the desired
mission-focused outcomes for IT investment decision making

■ Next Steps: the work that must still occur in order to successfully
implement an integrated approach to IT investment decision making

Page 13

Strategies at Work

Workshop Participants Identified Specific Success
Factors for Each Phase of the Select, Control,
Evaluate Model
Selec
¥ Establish corporate decision making infrastructure
¥ Involvement of functional level IT executives
¥ Use of scorecards
¥ Active, energized investment review board
¥ Use of Raines Rules and guiding principles
¥ Standardized reporting formats
¥ Exercised flexibility where appropriate
¥ Incorporated lessons learned into process
¥ Defined business, technical, and management goals and objectives
¥ Integrated IT planning cycle with agency budget cycle
¥ Developed portfolio management approaches
¥ Analyzed multiple investment risk categories
¥ Predicted benefits of investment that accrue in the near term rather
han 3-5 years
¥ Exercised a practical "make sense" approach
¥ Develop criteria for applying decision criteria
¥ Use mission based performance measurements

Control
¥ Document and simplify all Control/Evaluate findings
¥ Develop preview of milestones/review schedule
¥ View control as CEOÕs responsibility
¥ Review existing projects
¥ Relate frequency of reviews to level of investmen
¥ Remain faithful to the scheduled project reviews
¥ Use funding as a control mechanism
¥ Establish go/no go criteria for each review
¥ Develop an egress plan for termination
¥ Maximize use of existing formal control processes
¥ Structure integrated relationship with program management teams

Evaluate
¥ Establish thresholds for evaluations
¥ Critique, Select and Control phases during Evaluation
¥ Select the right staff to perform evaluations
¥ Ensure management involvemen
¥ Agree up front what is to be evaluated
¥ Incorporate evaluation results into overall IT business practices

Page 17

Implementing Best Practices

Workshop Participants Concluded That Success in the
Control and Evaluate Phases is Directly Dependen
Upon Developing Effective Decision Making Criteria,
Milestones, and Performance Measures During the Selec
Phase
■

Nearly every agency and organization reported success in implementing
portions of the Select phase by:
¥ Implementing investment review boards
¥ Developing IT decision criteria to assist in selecting IT investments
¥ Solidifying IT portfolios

■

Most agencies also indicated that they are still struggling with establishing
meaningful mission performance measures and developing effective
Return on Investment (ROI) cost models for existing and planned systems

■

Fewer agencies have made significant progress in the Control and Evaluate
phases
¥ Focus has been on developing the infrastructure needed to selec
IT investments
¥ Many agencies and organizations are implementing monitoring processes
in their investment review board structures

GAO Will be Looking for Tangible Evidence to
Determine Whether Agencies are Implementing
Effective Capital Planning and IT Investment Processes
■

Recommended reviewing GAOÕs guidebook: Assessing Risks and Returns:
A Guide for Evaluating Federal AgenciesÕ IT Investment Decision-Making,
located on the world wide web at: www.gao.gov/policy/itguide/index.htm

■

Provided insight on approaching ROI: "ROI creates a mindset and fosters
more informed decision making." (InfoWeek)
¥ Establish and implement well-defined, but simplified decision criteria
¥ Develop and implement a repeatable investment management process
¥ Use key department managers to verify that ROI goals have been
evaluated and me

■

Suggested, strongly, that agencies should develop a baseline of current IT
assets to assist in framing investment decisions

■

Noted that the legislation holds agency heads accountable for IT investment
decisions
¥ This is not always practical because system development often
is completed after agency leadership is gone
¥ The current trend, however; is the appointment of a growing number
of executives from the private sector who understand short ROI cycles

Page 18

Strategies at Work

Workshop Participants Also Identified barriers to
Successfully Implementing Effective Capital Planning
and IT Investment Processes
■

Gaining top-level attention, interest in IT

■

Organizational politics

■

Political danger of killing a bad project

■

CIO failure to sell importance of viewing IT acquisitions as investments

■

Turnover of political appointees (e.g., agency leadership)

■

Poor project management

■

Inadequate understanding of business and mission

■

Decentralized IT authority

■

Tendency to tinker during control phase, failure to take decisive action

■

Over-complicated processes and reporting procedures

■

Tendency to adopt one size fits all processes

■

Forecasting total life cycle cost

■

The difficulty of establishing clearly defined ROI measurements

■

Tendency to focus on price versus cost

Page 19

Implementing Best Practices

Participants Discussed Proactive Approaches That CIOs
Can Take to Mitigate Barriers and to Motivate Their
Organizations to View IT Acquisitions as Investments
and Mission Enablers
■

Understand the motivations of their leadership, craft strategies
and approaches to assist the leadership in accomplishing the business
objectives

■

Benchmark performance against the "best in class" and share results

■

Demonstrate value added to bottom line business need by IT investment

■

Educate project office and leadership about evaluation and results by
providing information to predict project success, based on data collected
during the Evaluation phase

Page 20

Strategies at Work

The Select, Control, Evaluate Model is Most Successful
When Intregrated with the Agency’s Overall Capital
Planning and Investment Process

Page 23

Implementing Best Practices

The Agencies With the Most
Mature Processes
Demonstrated Capital Planning
and IT Investment Processes
hat Were Integrated With the
Agency Mission

Best Practice – Case Study
## GSA
## NASA
DoD

■

Three agencies that have been noted as having integrated approaches for
making IT investment decisions provided workshop participants with details
of their IT decision-making process; they are presented in the following
pages as case studies
¥ General Services Administration
¥ National Aeronautics and Space Administration
¥ Department of Defense

■

The case studies provide background about each agency, including IT change
drivers, use the Select, Control, Evaluate Model as a framework, and focus on
six specific areas:

Processes
Process: the activities required to gather the information needed to make IT investment decisions

People
People: the committees, boards, and individuals involved in the decision-making
process

Tools and Techniques
Tools and Techniques: the automated and systematic approaches agencies are using
o assist in decision-making

Success Factors
Success Factors: the most important ingredients that led to successful implementation of the process

Challenges
Challenges: the roadblocks agencies are facing in achieving the desired missionfocused outcomes for IT investment decision-making

Next Steps
Next Steps: the work that must still occur in order to successfully implement an
integrated approach to IT investment decision-making
Page 24

Strategies at Work

Background: Growing Customer Best Practice – Case Study
Access to Outside Vendors
## GSA
Significantly Increased
## NASA
Competitive Pressures; GSA
Needed to Strengthen Their
DoD
Ability to Offer the Most CostEffective Products to Their
Customers
■

GSA took several significant steps to define their mission more tightly,
improve service to their customers, and ensure that GSA services were
cost-effective
¥ Initiated organization-wide BPR in 1993
¥ Reduced lines of business to 16
¥ Simplified purchasing and related regulations
¥ Delegated authority to agencies to lease real estate and purchase IT
and telecommunications services
¥ Revamped small purchase procedures and reduced order turnaround
ime
¥ Reduced workforce by 29%

■

GSA business, and ITÕs integration into the business, is characterized
by several factors:
¥ Only 1% ($155M) of the current agency budget of $13B is appropriated
¥ $155M appropriation represents 26% reduction since 1993
¥ "Knowing the business" is inculcated into management practices
at every level
¥ CIO is already "at the table" with the CEO and CFO
¥ Agency leadership has a clear view of associated IT investment risks
and anticipated rewards

Page 25

Implementing Best Practices

GSA IT Investment Process

es

nt
me

Inv
## INFORMATION TECHNOLOGY
COUNCIL (ITC) and COUNCIL OF
CONTROLLERS (COC)

n

Pla

Y
## E
## S

CUSTOMER
## CIO
Information Systems
Plan

CONTROL /
## EVALUATE BY
ITC/COC, BTC

N
## O
## SYSTEMS
DEVELOPEMENT &
## IMPLEMENTATION

BUSINESS TECHNOLOGY
COUNCIL (BTC)
ADMINISTRATOR'S
## DECISION
Approved Disapproved
David J. Barram

Page 26

CAPITAL
## PLAN
(Budget)

Strategies at Work

GSA’s Streamlined Approach to Selecting IT
Investments

• Criteria Information
• Criteria Point Values
• Scoring Rules
• Self-assessmen
Procedures

Develop
Investmen
Criteria

Agency-wide
Investmen
Methodology

Identify IT
Requiremen
Functional
Business Manager
Conduct IT
Investmen
Self
Assessmen

• Investment Description
• Mission Links
• Cost/Benefi
• Risk
• Overall Score

Generate
Rank-Ordered
Investmen
Lis

Approve IT
Portfolio

ITC/COC

Agency-level
Investmen
Resource Board (BTC)

Page 27

Implementing Best Practices

Select Phase: GSA Developed
Best Practice – Case Study
an IT Investment Portfolio
## GSA
Using Established Criteria and a
## NASA
Repeatable Process
Processes
DoD
■

Develop weighted criteria and scoring rules to identify major investments
hat would be reviewed using GSAÕs agency-wide process; criteria and rules
address:
¥ Dollar thresholds
¥ Legal requirements
¥ Mission criticality
¥ High executive interes
¥ Cross-functionality
¥ Benefit/cost ratio

■

Develop procedures for the functional business managers to conduct selfassessments of their IT investments

■

Conduct self-assessments to generate single investment scorecards

■

Generate a rank-ordered IT investment list

■

Develop and approve the IT investment portfolio at the agency level using
he scorecards and rank-ordered list as input to the decision-making process

Page 28

Strategies at Work

Select Phase: GSA Developed
an IT Investment Portfolio
Using Established Criteria and
a Repeatable Process (Cont’d)

Best Practice – Case Study
## GSA
## NASA
DoD

People
■

Functional Business Managers (Service/Staff Office)
¥ Conduct IT investment self-assessments using established criteria and
scoring rules
¥ Use a priority placement grid to determine relative priorities among the
IT investments

■

Information Technology Council (ITC)/Council of Controllers (COC)
¥ Chaired by Deputy CIO, participants include business area CIOs and
hree regional representatives; ITC meets regularly with COC
¥ Evaluates technical risk of investments by looking at BPR, organizational
impact, cost and schedule risks, resources and training issues
¥ Determines the implementation approach for IT investments

■

Business Technology Council (BTC)
¥ Reviews agency-level investments
¥ Consists of Administrator, Deputy Administrator, CIO, CFO, and
Service/Staff Office Heads
¥ Establishes priorities using the self-assessment inputs and makes final
funding decisions
¥ Reviews strategic alignment between investments and business goals

Page 29

Implementing Best Practices

Select Phase: GSA Developed
an IT Investment Portfolio
Using Established Criteria and
a Repeatable Process (Cont’d)

Best Practice – Case Study
## GSA
## NASA
DoD

Tools and Techniques
■

Integrated the investment selection process with the IT planning database
allowing OMB circular A-11, Exhibit 43 costs to be shown for each IT project,
for the overall business area, and for the agency IT portfolio

Success Factors
■

Emphasizing the meeting of business needs and requirements as the most
critical factors in the selection process

■

Involving the functional program managers in developing the IT
investment process

■

Engaging top-level management involvement throughout the ITC, COC
and BTC

■

Standardizing scoring rules and reporting formats for all users and providing
a concentrated and high level of hands-on assistance throughout the scoring
process

■

Conducting training on establishing IT-related performance goals and measures

■

Holding facilitated lessons-learned sessions with various levels at the end
of the Select phase

Challenges
■

Communicating with managers and users that the self-assessment numeric
score is not the single most important factor in making IT investment decisions,
but is only an inpu

■

Bringing IT and business communities together to establish functional and IT
performance measures

■

Obtaining realistic life cycle cost estimates and quantitative benefits

Next Steps
■

Integrate capital planning with other agency processes

■

Integrate lessons learned from FY99 budget process

Page 30

Strategies at Work

Control and Evaluate Phases:
Best Practice – Case Study
GSA has Several Project Review
## GSA
Mechanisms to Control and
## NASA
Evaluate an IT Investment’s
Progress
DoD
Processes
■

Establish Integrated Project Teams (IPTs) during the Select phase of each
projec
¥ IPTs provide direct project management responsibility
¥ Project managers provide monthly status reports and scorecard updates
hat focus on cost, schedule, deliverables, performance measures,
risk factors

■

Conduct Post Implementation Reviews (PIRs) on new systems within 3 to 6
months of implementation

■

Conducted periodic Operational Reviews on operating and infrastructure
systems

People
■

IPTs, with CIO as a member

■

ITC and BTC
¥ Receive status reports and scorecards
¥ ITC recommends and BTC decides to continue, modify, accelerate,
or cancel the projec

Tools and Techniques
■

IT planning database is in place and is being reviewed for expansion into
other phases of the Capital Planning Process

Page 31

Implementing Best Practices

Control and Evaluate Phases:
Best Practice – Case Study
GSA has Several Project Review
## GSA
Mechanisms to Control and
## NASA
Evaluate an IT Investment’s
Progress (Cont’d)
DoD
Success Factors
¥ Securing commitment to improving project management skills and to
accepting the team concept of project managemen
¥ Emphasizing the link between each Service or Staff Office IT Plan and the
AgencyÕs business needs and requirements
¥ Building on the performance measures identified in the Select phase

Challenges
¥ Obtaining managementÕs acceptance of the IPT concept of program
managemen
¥ Gaining Service/Staff Office acceptance of the ITC/BTC reviewing their
IT plans and having authority to approve or disapprove funding for
projects sponsored by the Service/Staff Office
¥ Accepting the move of the IT decision-making process from a
Service/Staff Office to an Agency focus

Next Steps
¥ Further institutionalize the control and evaluate methodologies
within GSA

Page 32

Strategies at Work

NASA’s Investment Process is Integrated with Strategic,
Financial, and Program Management Processes

•Establish IT Management
and Technical Goals/Objectives

• Agency Strategic
Plans
• Enterprise Strategic
Plans
• Agency 5-year
Budge

•Issue IT Budget Guidance
•Review IT Investmen
Plans & Performance Plans (Spring, Fall)
•Prepare Agency IT Plans/Budge

• Agency Budget Plans
POP Budget Guidance
• Agency Budge
• Agency Capital Plan
• Agency Performance
Plan

•Review IT Programs & Projects
•Review IT Component of Programs
& Projects
•Review IT Management Initiatives

•IT Performance
Repor

Planning
and
Budgeting

• Programs and
Projects Execution
• Program/Projec
Reviews

• Agency
Performance
Repor
• Program/Projec
Evaluations

Execution
and
Evaluation

Agency Profile
Agency Budget: $13.7B
IT Budget: $1.4B (90% is contracted)
Mission: Research and Developmen
Primary Businesses: 4
Footprint: National
Page 33

Implementing Best Practices

Background: NASA’s is
Best Practice – Case Study
Transitioning From Operational
## GSA
Endeavors to Becoming the
## NASA
Premier Scientific Research and
Development Agency
DoD
■

NASA is building on recent accomplishments with a renewed focus on
scientific research and development and application of new cutting-edge
echnologies
¥ NASA implemented a comprehensive, customer-focused strategic
planning and management process that established the AgencyÕs vision,
mission, and roadmap for the future
¥ NASAÕs Strategic Plan defined four primary enterprise businesses
Ð Mission to Planet Earth Enterprise
Ð Aeronautics and Space Transportation Technology Enterprise
Ð Human Exploration and Development of Space Enterprise
Ð Space Science Enterprise
¥ Information technology is part of the AgencyÕs cross-cut process
o manage strategically

■

NASAÕs approach to IT management is characterized by several factors:
¥ IT is viewed as an enabler to support space exploration, scientific
research, and technology development and transfer mission areas
¥ The Agency CIO reports to the Administrator and provides vision
and leadership on matters pertaining to IT plans, policies, standards,
investments, and assessments
¥ Enterprise-level CIOs and CIO Representatives at Centers are responsible
for ensuring effective IT management in a decentralized environmen

Page 34

Strategies at Work

NASA’s Intregrated IT With Program
Management Process

Agency Portfolio

Program
Managemen
Council(s)
Review all Programs
and Projects
• Formulation
• Approval
• Implementation
• Evaluation
• Technical Approach
• Budge
• Schedule

IT Programs/Projects
IT Components of
Programs/Projects

CIO

CIO Council

CIO Representatives

Information Technology
Investmen

Capital
Investmen
Council
• Information
Technology
• Facilities
• Infrastructure
• Evaluation
• Agency
Managemen
Initiatives

Multi-Enterprise
IT Infrastructure

Page 35

Implementing Best Practices

Select: NASA Integrates Their
Investment Process with Their
Strategic, Financial, and
Program Managemen
Processes

Best Practice – Case Study
## GSA
## NASA
DoD

Processes
■

Include IT management and technical goals in Agency Strategic Plan and
Enterprise Strategic Plans

■

Develop a two-pronged review process in which the Program Management
Council reviews all enterprise projects, including IT, and the Capital
Investment Council reviews IT infrastructure and multi-purpose IT projects

■

IT budget and investment planning and reporting processes are fully
integrated with the AgencyÕs overall budget process

■

All investments, including IT, are linked to mission performance using
strategic management framework

People
■

NASA CIO Council, chaired by NASA CIO, recommends Agency IT
investment strategies to Capital Investment Council
¥ Membership includes Enterprises and CFO

■

Program Management Council considers all programs and projects including
IT programs/projects and IT components of programs/projects
¥ Membership includes Enterprise Heads and CIO as an equal
participan

■

Capital Investment Council considers all multi-enterprise and IT infrastructure investments
¥ Membership includes the CFO, Deputy Administrator, and Enterprise Heads

Page 36

Strategies at Work

Select: NASA Integrates Their
Investment Process with Their
Strategic, Financial, and
Program Managemen
Processes (Cont’d)

Best Practice – Case Study
## GSA
## NASA
DoD

Tools and Techniques
■

Defined criteria for program formulation

■

Established management controls throughout program life-cycle to address
budget, technical, and schedule issues

■

Defined a hierarchy of investment reviews, including NASA CIO IT
Investment Review

Success Factors
■

Aligning the organization directly to a clearly defined mission

■

Integrating IT management processes with established Agency program and
project management processes and control

■

Adapting Raines Rules to meet unique ROI characteristics of research and
development programs

■

Coordinating closely with the CFO community

Challenges
■

Meeting IT needs and supporting mission with decreasing budget
¥ Maximizing outsourcing opportunities
¥ Consolidating IT resources (WANs, mainframe/mid-tier,
supercomputing)

■

Defining clear ROI criteria

Next Steps
■

Complete implementation of strategies for IT consolidations and outsourcing

■

Incorporate lessons learned from integrated process in future planning

Page 37

Implementing Best Practices

NASA Integrated IT With
Performance
Management Process

Agency Levels
## NASA
Strategic
Plan

NASA
Performance
Plan

NASA
Performance
Repor

IT Relative to Customer Satisfaction

High Performance
Computing:
“Demonstrate 10, 50, 100
gigaFLOPS performance”

Output

National Space Science
Data Center: “User
Committee Reviews”

Outcome

Service Levels
Spacecraf
Operations:
“Passes Taken
Successfully”

Customer Satisfaction

Investment Level

Page 38

Mission Control
Center: “Timely
Response to
Infligh
Anomalies”

Strategies at Work

Control and Evaluate Processes:
Best Practice – Case Study
NASA Integrates Their Selec
## GSA
Process With Their Performance
## NASA
Management Process
DoD
Process
■

At the highest level, NASA is using three documents to manage:
NASA Strategic Plan, NASA Performance Plan, and NASA Performance
Repor

■

IT performance measures are established at the agency level as part of the
NASA Performance Plan as well as at individual IT investment levels

■

AgencyÕs established program evaluation process used to ensure alignment
and integration of IT investments

■

A pilot program is being launched to measure Agency IT ROI relative to
customer satisfaction

People
■

Entire agency participates in processes

Tools and Techniques
■

NASA Performance Plan
¥ Performance Measures defined for IT as part of overall NASA
performance will be published in September 1997

■

Output, outcome, service level, and customer satisfaction performance
measures established at the individual IT investment level

■

Cost, schedule, and technical issues are reviewed as part of the Agency
program management evaluation process

Page 39

Implementing Best Practices

Control and Evaluate Processes:
NASA Integrates Their Selec
Process With Their
Performance Managemen
Process (Cont’d)

Best Practice – Case Study
## GSA
## NASA
DoD

Success Factors
■

Integration of control and evaluation processes with agency strategic, budget
and program management evaluation process

■

CIO Investment Reviews

Challenges
■

Quantifying intangibles (e.g., customer satisfaction)

Next Steps
■

Conduct pilot for Agency-level IT performance measure of ROI relative
o customer satisfaction

Page 40

Strategies at Work

Background: DoD’s Primary
Best Practice – Case Study
Challenge in Implementing the
## GSA
Tenets of the Clinger-Cohen Ac
## NASA
of 1996 Centered
on Integrating IT Decision
DoD
Points Into the Existing Capital
Planning and Investmen
Control Process
■

DoD reviewed its programs and mission during the Quadrennial Defense
Review (QDR) process
¥ The worldwide threat has changed dramatically over the last 10 years
¥ Budget decreases of $150 billion (38%) required streamlined processes
and businesses
¥ Staff reductions of 750,000 (33%) mandated increased efficiencies
¥ Advances in technology required DoD to begin adopting new processes
o ensure that U.S. national security interests will be protected
in the future

■

DoD had a long-standing capital planning and investment control process in
place that needed to be leveraged to effectively implement Clinger-Cohen

■

Acquisition reform has also contributed to improved IT decision-making
hrough consolidation of DoDÕs policies and directives
¥ Use of Integrated Product Teams (IPTs) facilitates issue identification and
provides risk mitigation for projects throughout the development stage
¥ Empowerment of program managers and oversight officials
¥ Tailoring oversight to individual characteristics of each program

Page 41

Implementing Best Practices

DoD’s Capital Planning and Investment Control
Process is known as the Planning, Programming,
and Budgeting System (PPBS)
Planning
Establish DoD Planning
and Programming
Guidance
Programming

Major Boards
• Defense Planning
Advisory Group

Evaluate Component
Program Objective
Memoranda (POMs)
for Consistency and
Compliance with DPG
& Fiscal Guidance

Result
Defense Planning
Guidance

Major Boards
• Defense Resources
Board (DRB)
• Program Review
Group (PRG)
Resul
Program Decision
Memoranda

Budgeting
Components Develop
Detailed Budgets Based
on Program Decision
Memoranda (PDMs)

Major Boards
• Not Applicable

Result
• PBDs
• Presidents Budge
Agency Profile
Agency Budget: $250 Billion
IT Budget: $10.4 Billion
Mission: Operational Readiness and R&D
Primary Businesses: Multiple
Footprint: Decentralized

Page 42

Strategies at Work

Select Phase: DoD’s IT
Investment Process is Tied to
he Programming and
Budgeting Processes

Best Practice – Case Study
## GSA
## NASA
DoD

Processes
■

Includes the DoD IT investment portfolio in the Planning, Programming,
Budgeting System (PPBS)
¥ Ensures correct selection of IT investments
¥ Evaluates IT investments
¥ Ensures success of IT investments

■

Uses the DoD ITM Strategic Plan as a planning tool for the overall
budget process
¥ Tied the ITM Strategic Plan to the GPRA Strategic Plan (the QDR)
and the Defense Planning Guidance
¥ Focused the Plan on improving mission performance

■

Takes a rigorous approach to linking all investments, including IT, to mission
performance by leveraging the existing PPBS Process
¥ DoD reviewed PPBS and continues to implement changes to strengthen
CIO participation (i.e., CIO co-chairs POM Issue Teams and participates
in the PRG and DRB)

■

Uses ROI as a major factor in IT investment decision making (economic
analysis, functional economic analysis)

People
■

Entire agency participation; all functionals in each Service and agency
participate

■

Each component agency also has its own hierarchical review process that
include equivalents to the DRB, PRG, and POM teams

Tools and Techniques
■

Used hierarchical reviews to accomplish review of the entire DoD budget,
including IT

Page 43

Implementing Best Practices

The IT Investment Process is Built Into the
Programming and Budgeting Processes
• Planning Guidance
• QDR
• DPG
• ITM Strategic Plan
Service and Agency Program Objective
Memoranda Build and Review
FY99-03 POM
OSD Staff Reviews
## POM

OSD Staff Develops
Issue Papers

POM Issue Team Review
(CIO Rep Co-chairs)
PRG Develops Recommendations
(Director, Program Analysis
and Evaluation Chairs,
CIO is Member)
DRB Decision
(DepSecDef Chairs
CIO by Invite)

Signed Program
Decision
Memorandum

Page 44

Strategies at Work

Control and Evaluate Phases:
The MAISRC Monitors and
Controls IT Investments
Throughout the Acquisition
Life Cycle

Best Practice – Case Study
## GSA
## NASA
DoD

Processes
■

Monitors and controls development of major AISs throughout the acquisition process

■

Oversees progress from one major decision point or milestone to another

■

Redirects or terminates programs if required

People
■

Chaired by the DoD CIO (ASD, C3I)

■

All stakeholders participate through Integrated Product Teams (IPTs) who
manage and oversee each project throughout the MAISRC process

Tools and Techniques
■ Used hierarchical reviews

Page 45

Implementing Best Practices

The Department of Defense Has a Structured Control
and Evaluation Process: Each Milestone Ends in a Program
Review

Milestone 0:
Concep
Exploration

MAISRC
(Milestone Decision Authority)
Chair: DoD CIO (ASD, C31)
Principal Members:
• USD (Comptroller)
• Joint Chiefs of Staff
• DOT&E
• DTSE&E
• DirectorAPI
• Deputy ASD (C31)
• User Representatives
• Senior Information
Management Official (s)
• Component Acquisition
Executives

Milestone I:
Approval to
Begin a New
Acquisition
Program

Milestone II:
Approval to Enter
Engineering and
Manufacturing
Developmen

Milestone III:
Engineering and
Manufacturing
Developmen

Page 46

Acquisition
Process

Strategies at Work

The Department of Defense
Has Been Successful in Refining
an Integrated Capital Planning
and IT Investment Process, bu
Continues to Face Challenges

Best Practice – Case Study
## GSA
## NASA
DoD

Success Factors
■

Develop and use the Strategic Plan - ensure guidance is reflected in budget

■

Support of the Agency head

■

Continued decisions to use the existing processes rather than reinvent
a process whenever problems arise

■

Capitalizing on the hierarchical review process in both PPBS and acquisition

■

Making hard decisions (e.g., MAISRC recently stopped the standard DoD
accounting system)

Challenges
■

Finding the right level of guidance - should not be too high level or too
detailed

■

Securing agreement from existing bodies to include the CIO in their forums

Next Steps
■

Improving link between MAISRC and PPBS processes

■

Improving portfolio analysis (i.e., relating IT to the mission area it supports)

Page 47

Implementing Best Practices

USDA Developed Draft Investment
Guidance That is Currently Being Used
by USDA’s Board Structure to Decide
on FY99 IT Investments
■

Best Practice
## USDA

Established a Departmental Executive Information Technology Investment
Review Board (EITRB)
¥ USDA CIO provides oversight and members include the Deputy
Secretary, Under Assistant Secretaries, and Departmental CIOs and CFOs
¥ Board reviews and approves IT investments using a ranking process
¥ Board considers additional factors to refine rankings

■

Established Agency (Bureau) Boards
¥ Participants include the Administrator, Deputy Administrators and CIO
¥ IT investment reviews are based on established criteria
¥ Program, budget, and IT personnel prepare the system project plan and
budget then submit them to the Board
Ð Conduct self-assessment of capital assets using a predetermined
set of criteria for risk and benefits
Ð Establish performance goals and measures
Ð Provide cross-cutting narrative for telecommunications costs
and IT staffing and locations
Ð Consider ITMRA requirements (e.g., Y2K)
¥ Boards approve and forward IT proposals to Departmen

■

Completed Guide to Capital Planning and Investment Control (Draft)
¥ Incorporates agency-wide planning into the Select, Control,
Evaluate Model
¥ Ensures IT investments are made to effectively support agency
mission objectives

Page 48

Strategies at Work

USDA’s Approach to Capital Planning and Investment
Control is Based on the Select, Control, Evaluate Model
Planning

Selection

Control

Evaluation

• Examine Strategic • Screen
Planning and
Program Linkage • Analyze

• Monitor Projects • Conduct Post
(Perform Variance Implementation
Anylysis)
Reviews

• Develop Baseline
Assessment and
Gap Analysis

• Take Corrective
Action

• Outline Functional
Requirements

• Prioritize
• Finalize
Decision

• Decide on
Adjustments
• Prepare "Lessons
Learned" Repor

• Identify and
Develop
Preliminary
Projects
• Use Input From
Evaluation Phase

Page 49

Implementing Best Practices

DOE is Refining its Approach for
Selecting IT Investments by Piloting
he I-TIPS System and its
Accompanying Policies and Procedures

■

Best Practice
## DOE

Incorporated BPR into IT investment process
¥ Established mission-based performance measures
¥ Conducted cost/benefit analysis
¥ Developed business decision criteria

■

Involved senior management involved in key processes
¥ Developed Corporate Investment Board (CIB): chaired by Deputy
Secretary, CIO serves as Executive Secretary, and membership includes
all Assistant Secretaries
¥ Met at scheduled intervals to discuss mission and strategy priorities

■

Developing a portfolio management system, I-TIPS, to measure business
benefit against technological risk
¥ Provides a tool to score IT investments
¥ Offers portfolio alternatives
¥ Identifies IT asset baseline
¥ Pilot tests being conducted for legacy, new, and concept systems
¥ Provides a tested, award winning selection model

Page 50

Strategies at Work

The Department of Energy is Developing an Automated
Portfolio Analysis Tool That Will Be the Focal Point for
he IT Decision-Making Process

Identified Thresholds
and Criteria for
Major IT Investments

• Total Life Cycle Costs:
Equipment and Infrastructure > $10 M;
Software > $2.5 M
• Criteria applied to Specific Systems
- High Visibility
- Legally required
- Cross-functional
- Mission Critical
Systems

Developed Guidance
o Score Investments

• Based on OMB
• Reflected GSA
Methodology

Assessed IT Portfolio
Using I-TIPS Approach

• Measured Business
Benefits Agains
Technical Risk
• Weighted
Questionnaire with
Sliding Scale

Developed IT Portfolio
Using Corporate
Investment Board

• Reviewed 47
Proposals
• Selected 21
• Tied to Overall Budge
• Tied to Monitoring
Evaluation Processes

Page 51

Implementing Best Practices

HUD
Modified
its
Established
Approach to IT Capital Planning to
SupportMajorBusinessTransformation
and Management Reform Within the
Department, and to Reflect the Direc
Involvement of the Agency Head in the
Capital Planning Process

Best Practice
## HUD

■

Defined the AgencyÕs Mission, Strategic Business
Plan, and Management Reform Plan (HUD 2020)

■

Identified three major systems initiatives that are critical to achieving
mission, strategic objectives, and management reform
¥ Three initiatives are a screen, or "prism" through which all proposed IT
Investments must be evaluated.

■

Created Technology Investment Board Working Group (TIBWG)
and Technology Investment Board Executive Committee (TIBEC)
o perform capital planning functions previously performed by TIB
and Management Committee
¥ HUD Secretary, Deputy Secretary, and executive Principal Staff actively
participate in monthly TIBEC meetings
¥ The "prism" initiative project leaders and teams evaluate, rank, and select,
proposed investments which support HUD 2020 reforms
¥ COO, CIO, CFO, and "prism" Project Leaders determine proposed
funding for recommended investments
¥ TIBEC has final approval

■

Success Factors
¥ Having an Agency Head that is mission focused and highly interested
in understanding the contribution each investment, including IT,
will make to the mission
¥ Focusing on results
¥ Developing flexible processes that enable sound decisions that cut across
raditional program cylinders

Page 52

Strategies at Work

Housing and Urban Development’s Selection Process for IT
Investments is Modified to Support Agency Management Reform

Page 53

Implementing Best Practices

The State Department has Established
an Information Resources Managemen
Program Board (IRMB)
■

Best Practice
State

Established the IRMB to prioritize IT investments and provide management
oversigh
¥ Considered Raines Rules and Pesky questions
¥ Investigated use of COTS for ranking and scoring process
(e.g. Expert Choice)
¥ Identified interdependencies and dependencies
¥ Examined ROI goals, objectives

■

Acted on initiatives generated through the planning process
¥ Y2K compliance

■

Implemented group decision making methodology

■

Involved key management decision makers
¥ CIO, CFO, A/S for Administration, Executive Secretary, A/S
for Diplomatic Security, DAS for IM, Procurement Executive, Deputy
Legal Advisor, Director, Officer of Acquisitions, A/S (Functional Bureau)

■

Authored post implementation review and evaluation criteria

■

Developed a plan to evolve the ranking and scoring process using an
automated tool

Page 54

Strategies at Work

The Department of State is Integrating IT Planning
and Agency Performance Measurements

• Incorporated Agency Mission Imperatives
-Foreign Affairs Goals
-Diplomatic Readiness

• Authored Business Case
• Used Metrics Analysis

Developed
Dept.-Wide
Strategic and
Performance
Measuremen
Plan

Developed
Bureau-Specific
Perfromance
Plans

• Developed IT Performance Metrics

Constructed
Dept.-Wide IRM
Performance
Measuremen
Plan

• Examined Ability of IT as Business
Case Enabler

Developed
IRM Tactical Plan

• Project Plans
• Implementation On Plan

Developed
IRM Operational
Plan

Page 55

Implementing Best Practices

EPA Strengthened an Existing
Infrastructure for IT Capital Planning
and Investmen
■

Best Practice
## EPA

Focused on the Strategic Role of the CIO
¥ Established effective relationship with program offices and with CFO
¥ Installed CIO as Chair of Executive Committee for IRM
¥ Created appropriate investment review and evaluation protocols
¥ Worked to increase CFO understanding of IT capabilities

■

Developed a flexible investment process
¥ Tied investments directly to mission needs
¥ Incorporated direct input from program offices (Subcommittee on IT
Investment Review)

■

Defined a clear portfolio recommendation format
¥ Green Light = Strong recommendation for funding/implementation
¥ Yellow Light = IT project requires further evaluation agains
predetermined criteria and external factors
¥ Red Light = IT project is not ready to go forward to the CFO
Budget Process

Page 56

Strategies at Work

The Environmental Protection Agency’s Approach to IT
Investment Planning and Selection Includes a Red,Yellow,
Green-Light Evaluation Proir to Funding the Initiative

Identify IT Investment Priorities
• Sponsored by Program
Offices, Tied to Mission
Needs
• Involved EPA CIO in
DevelopingPortfolio
• Used Raines Rules
• Established Thresholds
- $1M/Year or $5M
Life Cycle
- Mission Critical in
More Than
Business Area

Review Investments

Complete Budget Process

• Identified Portfolio Using • Evaluated Green Light
Projects for Funding
he Executive Steering
Committee for IRM
• Provided Input for Further
- Chaired by CIO
Evaluating Yellow
- Membership
Light Projects
includes AA, IG,
CFO, OGC
• Offered ROI, Other Performance Evaluations
• Received Input From Subcommittee on IT Investmen
Review
- Co-chaired by
DCIO, DCFO
- Membership Includes
Strong Program and
Regional participation

• Included BPR
• Developed Risk Management Criteria

• Make Recommendations
- Green Light (36 of 44)
- Yellow Light (2 of 44)
- Red Light (6 of 44)

Page 57

Implementing Best Practices

The Coast Guard Developed a Set of
Criteria That Fall Into Four Categories:
Risk, Mission Effectiveness, Strategic
and Operational Alignment, and
Benefit/Cost Impacts
■

Best Practice
## USCG

Involved designated personnel and offices in the process
¥ Program (business area) develops IT budgetary and support documentation
¥ IRM Peer Group uses defined set of criteria and scoring rules to
evaluate and rank IT investments
¥ IRM Board determines final recommended IT portfolio for inclusion
in the agencyÕs budge
Ð Board members include senior agency managers and USCG CIO

■

Developed corporate information infrastructure to meet process demands
¥ Created an automated in-house application that assists IRM Peer Group
in scoring and extrapolating raw scores in the development of IT portfolio

■

Recognized Implementation Barriers
¥ Acceptance of program offices
¥ Gaining senior management suppor
¥ Personnel turnover
¥ Maintaining process integrity and clarity

■

Highlighted Success Factors
¥ Ensured support throughout agency by including program offices
at the IRM Peer Group and IRM Board levels
¥ Provided level playing field for evaluating competing IT investments
¥ Recognized GAO and OMB guidance and interest in IT investmen
planning reform

Page 58

Strategies at Work

U.S. Coast Guard Developed IT Investment Weighting
Criteria that Incorporates Concepts of GAO “Bes
Practices” for IT Planning and the OMB Raines Rules

Page 59

Implementing Best Practices

Department of Commerce
and Nuclear Regulatory Commission
Outlined Best Practices Implemented
in the Select Phase of IT Capital
Planning
■

Best Practice
Commerce

NRC

Department of Commerce established the Commerce Information Technology
Review Board (CITRB), chaired by the CIO
¥ Worked with budget staff to coordinate and combine requests for budge
and IT plan submissions from all Commerce operating units
¥ Required sponsors of major IT initiatives to brief Departmental Staff
on the following areas for each investment: mission relevance, Y2K
compliance, ROI, risk mitigation
¥ Used Raines Rules to review operating unit IT plans
¥ Ensured IT investments are aligned with department strategy;
satisfy mission requirements; are compatible with architecture goals;
minimize risk; and meet ROI requirements
¥ Developed an IT capital planning and investment processes that buil
upon GAO and OMB guidance and comments from the Commerce CIO
Council and budget community
¥ Developed decision criteria and scoring processes for ranking new
IT projects and modifications to existing systems, used the criteria
in a trial process to score and prioritize projects, and revise criteria
o incorporate lessons learned
¥ Programmed future revisions in decision criteria to reflect lessons learned
from trial

■

NRC Developed detailed guides for IT cost/benefit and risk analysis
¥ Formed program and IT staff teams to evaluate proposed IT investments
and write executive summaries for top-level managemen

Page 60

Appendix A:
List of Attendees

Strategies at Work

Appendix A

List of Attendees
Agency Web Sites
Agency

Web Site

USDA

www.usda.gov

Commerce

www.doc.gov

DoD

www.dtic.mil/c3i/

EPA

www.epa.gov

DOE

www.doe.gov

GAO

www.gao.gov

GSA

www.gsa.gov

NASA

www.nasa.gov

HUD

www.hud.gov

State

www.state.gov

USCG

www.dot.gov/dotinfo/uscg

FDA

www.fda.gov/

SSA

www.ssa.gov/

Treasury

www.ustreas.gov/

US Army

www.army.mil/

VA

www.va.gov/

NIH

www.nih.gov/

HHS

www.os.hhs.gov/

DoED

www.ed.gov/

NRC

www.nrc.gov/

RS

www.irs.ustreas.gov/

FEMA

www.fema.gov/

Page A3

Appendix B:
List of Acronyms

Appendix B

Acronym List
## IRM

Information Resources Management

IT

Information Technology

ITC

Information Technology Council

ITM

Information Technology Management

ITMRA

Information Technology Management Reform Act

MAISRC

Major AIS Resource Council

NASA

National Aeronautics and Space Administration

NRC

Nuclear Regulatory Commission

OMB

Office of Management and Budget

PA&E

Program Analysis and Evaluation

PDM

Program Decision Memoranda

PIR

Post Implementation Review

POM

Program Objective Memoranda

PPBS

Planning, Programming, and Budgeting System

PRG

Program Review Group

QDR

Quadrennial Defense Review

R&D

Research and Development

ROI

Return on Investment

USCG

U.S. Coast Guard

USDA

Department of Agriculture

WAN

Wide Area Network

Y2K

Year 2000

Page B3

